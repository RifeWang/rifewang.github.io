<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>凌虚 Blog</title><link>https://rifewang.github.io/</link><description>Recent content on 凌虚 Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 15 Apr 2022 00:00:00 +0800</lastBuildDate><atom:link href="https://rifewang.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于我</title><link>https://rifewang.github.io/about/</link><pubDate>Tue, 14 Feb 2023 17:03:10 +0800</pubDate><guid>https://rifewang.github.io/about/</guid><description>本人具备多年项目经验，目前负责后端开发与系统架构，坐标杭州。我的博客将会持续分享有关 Node.js，Python，Golang，编程，应用开发，消息队列，中间件，数据库，容器化，云原生，大数据，图像处理，机器学习，人工智能，架构，程序员成长，等等一系列文章。
⭐ I am RifeWang Link to heading 🧑‍💻 A back-end software developer and system architect. ❤️ Love Node.js and Go, good at Elasticsearch and Kubernetes. 🏠 Currently working and living in Hangzhou, China. 💬 You can communicate technical issues with me through my WeChat: rifewang . 🛠 My Tech Stack Link to heading 💻 🌐 🛢 🔧 欢迎关注我的微信公众号，并与我交流：</description></item><item><title>Elasticsearch 向量搜索</title><link>https://rifewang.github.io/posts/elasticsearch/es-vector-search/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-vector-search/</guid><description>Elasticsearch 向量搜索 Link to heading 本文将会介绍 Elasticsearch 向量搜索的两种方式。
向量搜索 Link to heading 提到向量搜索，我想你一定想知道：
向量搜索是什么？ 向量搜索的应用场景有哪些？ 向量搜索与全文搜索有何不同？ ES 的全文搜索简而言之就是将文本进行分词，然后基于词通过 BM25 算法计算相关性得分，从而找到与搜索语句相似的文本，其本质上是一种 term-based（基于词）的搜索。
全文搜索的实际使用已经非常广泛，核心技术也非常成熟。但是，除了文本内容之外，现实生活中还有非常多其它的数据形式，例如：图片、音频、视频等等，我们能不能也对这些数据进行搜索呢？
答案是 Yes !
随着机器学习和人工智能等技术的发展，万物皆可 Embedding。换句话说就是，我们可以对文本、图片、音频、视频等等一切数据通过 Embedding 相关技术将其转换成特征向量，而一旦向量有了，向量搜索的需求随之也越发强烈，向量搜索的应用场景也变得一望无际、充满想象力。
ES 向量搜索说明 Link to heading ES 向量搜索目前有两种方式:
script_score _knn_search script_score 精确搜索 Link to heading ES 7.6 版本对新增的字段类型 dense_vector 确认了稳定性保证，这个字段类型就是用来表示向量数据的。
数据建模示例：
PUT my-index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_vector&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;dense_vector&amp;#34;, &amp;#34;dims&amp;#34;: 128 }, &amp;#34;my_text&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;keyword&amp;#34; } } } } 如上图所示，我们在索引中建立了一个 dims 维度为 128 的向量数据字段。</description></item><item><title>Terraform: 基础设施即代码</title><link>https://rifewang.github.io/posts/devops/terraform-overview/</link><pubDate>Sun, 27 Mar 2022 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/devops/terraform-overview/</guid><description>Terraform: 基础设施即代码 Link to heading 问题 Link to heading 现如今有很多 IT 系统的基础设施直接使用了云厂商提供的服务，假设我们需要构建以下基础设施：
VPC 网络 虚拟主机 负载均衡器 数据库 文件存储 &amp;hellip; 那么在公有云的环境中，我们一般怎么做？
在云厂商提供的前端管理页面上手动操作吗？
这也太费劲了吧，尤其是当基础设施越来越多、越来越复杂、以及跨多个云环境的时候，这些基础设施的配置和管理便会碰到一个巨大的挑战。
Terraform Link to heading 为了解决上述问题，Terrafrom 应运而生。
使用 Terraform ，我们只需要编写简单的声明式代码，形如：
... resource &amp;#34;alicloud_db_instance&amp;#34; &amp;#34;instance&amp;#34; { engine = &amp;#34;MySQL&amp;#34; engine_version = &amp;#34;5.6&amp;#34; instance_type = &amp;#34;rds.mysql.s1.small&amp;#34; instance_storage = &amp;#34;10&amp;#34; ... } 然后执行几个简单的 terraform 命令便可以轻松创建一个阿里云的数据库实例。
这就是 Infrastructure as code 基础设施即代码。也就是通过代码而不是手动流程来管理和配置基础设施。
正如其官方文档所述，与手动管理基础设施相比，使用 Terraform 有以下几个优势：
Terraform 可以轻松管理多个云平台上的基础设施。 使用人类可读的声明式的配置语言，有助于快速编写基础设施代码。 Terraform 的状态允许您在整个部署过程中跟踪资源更改。 可以对这些基础设施代码进行版本控制，从而安全地进行协作。 Provider &amp;amp; Module Link to heading 你也许会感到困惑，我只是简单的应用了所写的声明式代码，怎么就构建出来了基础设施，这中间发生了什么？</description></item><item><title>加速 Kubernetes 镜像拉取</title><link>https://rifewang.github.io/posts/kubernetes/speed-up-image-pull/</link><pubDate>Sun, 13 Mar 2022 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/kubernetes/speed-up-image-pull/</guid><description>加速 Kubernetes 镜像拉取 Link to heading Kubernetes pod 启动时会拉取用户指定的镜像，一旦这个过程耗时太久就会导致 pod 长时间处于 pending 的状态，从而无法快速提供服务。
镜像拉取的过程参考下图所示：
Pod 的 imagePullPolicy 镜像拉取策略有三种：
IfNotPresent：只有当镜像在本地不存在时才会拉取。 Always：kubelet 会对比镜像的 digest ，如果本地已缓存则直接使用本地缓存，否则从镜像仓库中拉取。 Never：只使用本地镜像，如果不存在则直接失败。 说明：每个镜像的 digest 一定唯一，但是 tag 可以被覆盖。
从镜像拉取的过程来看，我们可以从以下三个方面来加速镜像拉取：
缩减镜像大小： 使用较小的基础镜像、移除无用的依赖、减少镜像 layer 、使用多阶段构建等等。 推荐使用 docker-slim 加快镜像仓库与 k8s 节点之间的网络传输速度。 主动缓存镜像： Pre-pulled 预拉取镜像，以便后续直接使用本地缓存，比如可以使用 daemonset 定期同步仓库中的镜像到 k8s 节点本地。 题外话 1：本地镜像缓存多久？是否会造成磁盘占用问题？
本地缓存的镜像一定会占用节点的磁盘空间，也就是说缓存的镜像越多，占用的磁盘空间越大，并且缓存的镜像默认一直存在，并没有 TTL 机制（比如说多长时间以后自动过期删除）。
但是，k8s 的 GC 机制会自动清理掉镜像。当节点的磁盘使用率达到 HighThresholdPercent 高百分比阈值时（默认 85% ）会触发垃圾回收，此时 kubelet 会根据使用情况删除最旧的不再使用的镜像，直到磁盘使用率达到 LowThresholdPercent（默认 80% ）。
题外话 2：镜像 layer 层数真的越少越好吗？
我们经常会看到一些文章说在 Dockerfile 里使用更少的 RUN 命令之类的减少镜像的 layer 层数然后缩减镜像的大小，layer 越少镜像越小这确实没错，但是某些场景下得不偿失。首先，如果你的 RUN 命令很大，一旦你修改了其中某一个小的部分，那么这个 layer 在构建的时候就只能重新再来，无法使用任何缓存；其次，镜像的 layer 在上传和下载的过程中是可以并发的，而单独一个大的层无法进行并发传输。</description></item><item><title>web 安全系列文章【译文】</title><link>https://rifewang.github.io/posts/linkto/web-security/</link><pubDate>Thu, 12 Aug 2021 10:58:20 +0800</pubDate><guid>https://rifewang.github.io/posts/linkto/web-security/</guid><description> Cross-site request forgery (CSRF) Link to heading CSRF XSS vs CSRF CSRF tokens SameSite cookies Clickjacking (UI redressing) Link to heading Clickjacking (UI redressing) Cross-origin resource sharing (CORS) Link to heading CORS Same-origin policy (SOP) Access-control-allow-origin Server-side request forgery (SSRF) Link to heading Server-side request forgery (SSRF) Blind SSRF vulnerabilities HTTP request smuggling Link to heading HTTP request smuggling Finding HTTP request smuggling vulnerabilities Exploiting HTTP request smuggling vulnerabilities OS command injectionn Link to heading OS command injection Server-side template injection Link to heading Server-side template injection Exploiting server-side template injection vulnerabilities Directory traversal Link to heading Directory traversal DOM-based vulnerabilities Link to heading DOM-based vulnerabilities DOM clobbering HTTP Host header attacks Link to heading HTTP Host header attacks Exploiting HTTP Host header vulnerabilities Password reset poisoning</description></item><item><title>web 安全之 Server-side template injection</title><link>https://rifewang.github.io/translation/web-security/server-side-template-injection/server-side-template-injection/</link><pubDate>Wed, 10 Mar 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/server-side-template-injection/server-side-template-injection/</guid><description>Server-side template injection Link to heading 在本节中，我们将介绍什么是服务端模板注入，并概述利用此漏洞的基本方法，同时也将提供一些避免此漏洞的建议。
什么是服务端模板注入 Link to heading 服务端模板注入是指攻击者能够利用模板自身语法将恶意负载注入模板，然后在服务端执行。
模板引擎被设计成通过结合固定模板和可变数据来生成网页。当用户输入直接拼接到模板中，而不是作为数据传入时，可能会发生服务端模板注入攻击。这使得攻击者能够注入任意模板指令来操纵模板引擎，从而能够完全控制服务器。顾名思义，服务端模板注入有效负载是在服务端交付和执行的，这可能使它们比典型的客户端模板注入更危险。
服务端模板注入会造成什么影响 Link to heading 服务端模板注入漏洞会使网站面临各种攻击，具体取决于所讨论的模板引擎以及应用程序如何使用它。在极少数情况下，这些漏洞不会带来真正的安全风险。然而，大多数情况下，服务端模板注入的影响可能是灾难性的。
最严重的情况是，攻击者有可能完成远程代码执行，从而完全控制后端服务器，并利用它对内部基础设施进行其他攻击。
即使在不可能完全执行远程代码的情况下，攻击者通常仍可以使用服务端模板注入作为许多其他攻击的基础，从而可能获得服务器上敏感数据和任意文件的访问权限。
服务端模板注入漏洞是如何产生的 Link to heading 当用户输入直接拼接到模板中而不是作为数据传入时，就会出现服务端模板注入漏洞。
简单地提供占位符并在其中呈现动态内容的静态模板通常不会受到服务端模板注入的攻击。典型的例子如提取用户名作为电子邮件的开头，例如以下从 Twig 模板中提取的内容：
$output = $twig-&amp;gt;render(&amp;#34;Dear {first_name},&amp;#34;, array(&amp;#34;first_name&amp;#34; =&amp;gt; $user.first_name) ); 这不容易受到服务端模板注入的攻击，因为用户的名字只是作为数据传递到模板中的。
但是，Web 开发人员有时可能将用户输入直接连接到模板中，如：
$output = $twig-&amp;gt;render(&amp;#34;Dear &amp;#34; . $_GET[&amp;#39;name&amp;#39;]); 此时，不是将静态值传递到模板中，而是使用 GET name 动态生成模板本身的一部分。由于模板语法是在服务端执行的，这可能允许攻击者使用 name 参数如下：
http://vulnerable-website.com/?name={{bad-stuff-here}} 像这样的漏洞有时是由于不熟悉安全概念的人设计了有缺陷的模板造成的。与上面的例子一样，你可能会看到不同的组件，其中一些组件包含用户输入，连接并嵌入到模板中。在某些方面，这类似于 SQL 注入漏洞，都是编写了不当的语句。
然而，有时这种行为实际上是有意为之。例如，有些网站故意允许某些特权用户（如内容编辑器）通过设计来编辑或提交自定义模板。如果攻击者能够利用特权帐户，这显然会带来巨大的安全风险。
构造服务端模板注入攻击 Link to heading 识别服务端模板注入漏洞并策划成功的攻击通常涉及以下抽象过程。
探测 Link to heading 服务端模板注入漏洞常常不被注意到，这不是因为它们很复杂，而是因为它们只有在明确寻找它们的审计人员面前才真正明显。如果你能够检测到存在漏洞，则利用它将非常容易。在非沙盒环境中尤其如此。
与任何漏洞一样，利用漏洞的第一步就是先找到它。也许最简单的初始方法就是注入模板表达式中常用的一系列特殊字符，例如 ${{&amp;lt;%[%&amp;rsquo;&amp;quot;}}%\ ，去尝试模糊化模板。如果引发异常，则表明服务器可能以某种方式解释了注入的模板语法，从而表明服务端模板注入可能存在漏洞。</description></item><item><title>Exploiting server-side template injection vulnerabilities</title><link>https://rifewang.github.io/translation/web-security/server-side-template-injection/exploiting-server-side-template-injection-vulnerabilities/</link><pubDate>Wed, 10 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/server-side-template-injection/exploiting-server-side-template-injection-vulnerabilities/</guid><description>利用服务端模板注入漏洞 Link to heading 在本节中，我们将更仔细地了解一些典型的服务端模板注入漏洞，并演示如何利用之前归纳的方法。通过付诸实践，你可以潜在地发现和利用各种不同的服务端模板注入漏洞。
一旦发现服务端模板注入漏洞，并确定正在使用的模板引擎，成功利用该漏洞通常涉及以下过程。
阅读 模板语法 安全文档 已知的漏洞利用 探索环境 构造自定义攻击 阅读 Link to heading 除非你已经对模板引擎了如指掌，否则应该先阅读其文档。虽然这可能有点无聊，但是不要低估文档可能是有用的信息来源。
学习基本模板语法 Link to heading 学习基本语法、关键函数和变量处理显然很重要。即使只是简单地学习如何在模板中嵌入本机代码块，有时也会很快导致漏洞利用。例如，一旦你知道正在使用基于 Python 的 Mako 模板引擎，实现远程代码执行可以简单到：
&amp;lt;% import os x=os.popen(&amp;#39;id&amp;#39;).read() %&amp;gt; ${x} 在非沙盒环境中，实现远程代码执行并将其用于读取、编辑或删除任意文件在许多常见模板引擎中都非常简单。
阅读安全部分 Link to heading 除了提供如何创建和使用模板的基础知识外，文档还可能提供某种“安全”部分。这个部分的名称会有所不同，但它通常会概括出人们应该避免使用模板进行的所有潜在危险的事情。这可能是一个非常宝贵的资源，甚至可以作为一种备忘单，为你应该寻找哪些行为，以及如何利用它们提供指南。
即使没有专门的“安全”部分，如果某个特定的内置对象或函数会带来安全风险，文档中几乎总是会出现某种警告。这个警告可能不会提供太多细节，但至少应将其标记为可以深入挖掘研究的内容。
例如，在 ERB 模板中，文档显示可以列出所有目录，然后按如下方式读取任意文件：
&amp;lt;%= Dir.entries(&amp;#39;/&amp;#39;) %&amp;gt; &amp;lt;%= File.open(&amp;#39;/example/arbitrary-file&amp;#39;).read %&amp;gt; 查找已知的漏洞利用 Link to heading 利用服务端模板注入漏洞的另一个关键方面是善于查找其他在线资源。一旦你能够识别正在使用的模板引擎，你应该浏览 web 以查找其他人可能已经发现的任何漏洞。由于一些主要模板引擎的广泛使用，有时可能会发现有充分记录的漏洞利用，你可以对其进行调整以利用到自己的目标网站。
探索 Link to heading 此时，你可能已经在使用文档时偶然发现了一个可行的漏洞利用。如果没有，下一步就是探索环境并尝试发现你可以访问的所有对象。
许多模板引擎公开某种类型的 self 或 environment 对象，其作用类似于包含模板引擎支持的所有对象、方法和属性的命名空间。如果存在这样的对象，则可以潜在地使用它来生成范围内的对象列表。例如，在基于 Java 的模板语言中，有时可以使用以下注入列出环境中的所有变量：
${T(java.lang.System).getenv()} 这可以作为创建一个潜在有趣对象和方法的短名单的基础，以便进一步研究。</description></item><item><title>web 安全之 CSRF</title><link>https://rifewang.github.io/translation/web-security/csrf/csrf/</link><pubDate>Tue, 09 Mar 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/csrf/csrf/</guid><description>Cross-site request forgery (CSRF) Link to heading 在本节中，我们将解释什么是跨站请求伪造，并描述一些常见的 CSRF 漏洞示例，同时说明如何防御 CSRF 攻击。
什么是 CSRF Link to heading 跨站请求伪造（CSRF）是一种 web 安全漏洞，它允许攻击者诱使用户执行他们不想执行的操作。攻击者进行 CSRF 能够部分规避同源策略。
CSRF 攻击能造成什么影响 Link to heading 在成功的 CSRF 攻击中，攻击者会使受害用户无意中执行某个操作。例如，这可能是更改他们帐户上的电子邮件地址、更改密码或进行资金转账。根据操作的性质，攻击者可能能够完全控制用户的帐户。如果受害用户在应用程序中具有特权角色，则攻击者可能能够完全控制应用程序的所有数据和功能。
CSRF 是如何工作的 Link to heading 要使 CSRF 攻击成为可能，必须具备三个关键条件：
相关的动作。攻击者有理由诱使应用程序中发生某种动作。这可能是特权操作（例如修改其他用户的权限），也可能是针对用户特定数据的任何操作（例如更改用户自己的密码）。 基于 Cookie 的会话处理。执行该操作涉及发出一个或多个 HTTP 请求，应用程序仅依赖会话cookie 来标识发出请求的用户。没有其他机制用于跟踪会话或验证用户请求。 没有不可预测的请求参数。执行该操作的请求不包含攻击者无法确定或猜测其值的任何参数。例如，当导致用户更改密码时，如果攻击者需要知道现有密码的值，则该功能不会受到攻击。 假设应用程序包含一个允许用户更改其邮箱地址的功能。当用户执行此操作时，会发出如下 HTTP 请求：
POST /email/change HTTP/1.1 Host: vulnerable-website.com Content-Type: application/x-www-form-urlencoded Content-Length: 30 Cookie: session=yvthwsztyeQkAPzeQ5gHgTvlyxHfsAfE email=wiener@normal-user.com 这个例子符合 CSRF 要求的条件：
更改用户帐户上的邮箱地址的操作会引起攻击者的兴趣。执行此操作后，攻击者通常能够触发密码重置并完全控制用户的帐户。 应用程序使用会话 cookie 来标识发出请求的用户。没有其他标记或机制来跟踪用户会话。 攻击者可以轻松确定执行操作所需的请求参数的值。 具备这些条件后，攻击者可以构建包含以下 HTML 的网页：</description></item><item><title>CSRF tokens</title><link>https://rifewang.github.io/translation/web-security/csrf/csrf-tokens/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/csrf/csrf-tokens/</guid><description>CSRF tokens Link to heading 在本节中，我们将解释什么是 CSRF token，它们是如何防御的 CSRF 攻击，以及如何生成和验证CSRF token 。
什么是 CSRF token Link to heading CSRF token 是一个唯一的、秘密的、不可预测的值，它由服务端应用程序生成，并以这种方式传输到客户端，使得它包含在客户端发出的后续 HTTP 请求中。当发出后续请求时，服务端应用程序将验证请求是否包含预期的 token ，并在 token 丢失或无效时拒绝该请求。
由于攻击者无法确定或预测用户的 CSRF token 的值，因此他们无法构造出一个应用程序验证所需全部参数的请求。所以 CSRF token 可以防止 CSRF 攻击。
CSRF token 应该如何生成 Link to heading CSRF token 应该包含显著的熵，并且具有很强的不可预测性，其通常与会话令牌具有相同的特性。
您应该使用加密强度伪随机数生成器（PRNG），该生成器附带创建时的时间戳以及静态密码。
如果您需要 PRNG 强度之外的进一步保证，可以通过将其输出与某些特定于用户的熵连接来生成单独的令牌，并对整个结构进行强哈希。这给试图分析令牌的攻击者带来了额外的障碍。
如何传输 CSRF token Link to heading CSRF token 应被视为机密，并在其整个生命周期中以安全的方式进行处理。一种通常有效的方法是将令牌传输到使用 POST 方法提交的 HTML 表单的隐藏字段中的客户端。提交表单时，令牌将作为请求参数包含：
&amp;lt;input type=&amp;#34;hidden&amp;#34; name=&amp;#34;csrf-token&amp;#34; value=&amp;#34;CIwNZNlR4XbisJF39I8yWnWX9wX4WFoz&amp;#34; /&amp;gt; 为了安全起见，包含 CSRF token 的字段应该尽早放置在 HTML 文档中，最好是在任何非隐藏的输入字段之前，以及在 HTML 中嵌入用户可控制数据的任何位置之前。这可以对抗攻击者使用精心编制的数据操纵 HTML 文档并捕获其部分内容的各种技术。</description></item><item><title>SameSite cookies</title><link>https://rifewang.github.io/translation/web-security/csrf/samesite-cookies/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/csrf/samesite-cookies/</guid><description>SameSite cookies Link to heading 某些网站使用 SameSite cookies 防御 CSRF 攻击。
这个 SameSite 属性可用于控制是否以及如何在跨站请求中提交 cookie 。通过设置会话 cookie 的属性，应用程序可以防止浏览器默认自动向请求添加 cookie 的行为，而不管cookie 来自何处。
这个 SameSite 属性在服务器的 Set-Cookie 响应头中设置，该属性可以设为 Strict 严格或者 Lax 松懈。例如：
SetCookie: SessionId=sYMnfCUrAlmqVVZn9dqevxyFpKZt30NN; SameSite=Strict; SetCookie: SessionId=sYMnfCUrAlmqVVZn9dqevxyFpKZt30NN; SameSite=Lax; 如果 SameSite 属性设置为 Strict ，则浏览器将不会在来自其他站点的任何请求中包含cookie。这是最具防御性的选择，但它可能会损害用户体验，因为如果登录的用户通过第三方链接访问某个站点，那么他们将不会登录，并且需要重新登录，然后才能以正常方式与站点交互。
如果 SameSite 属性设置为 Lax ，则浏览器将在来自另一个站点的请求中包含cookie，但前提是满足以下两个条件：
请求使用 GET 方法。使用其他方法（如 POST ）的请求将不会包括 cookie 。 请求是由用户的顶级导航（如单击链接）产生的。其他请求（如由脚本启动的请求）将不会包括 cookie 。 使用 SameSite 的 Lax 模式确实对 CSRF 攻击提供了部分防御，因为 CSRF 攻击的目标用户操作通常使用 POST 方法实现。这里有两个重要的注意事项：
有些应用程序确实使用 GET 请求实现敏感操作。 许多应用程序和框架能够容忍不同的 HTTP 方法。在这种情况下，即使应用程序本身设计使用的是 POST 方法，但它实际上也会接受被切换为使用 GET 方法的请求。 出于上述原因，不建议仅依赖 SameSite Cookie 来抵御 CSRF 攻击。当其与 CSRF token 结合使用时，SameSite cookies 可以提供额外的防御层，并减轻基于令牌的防御中的任何缺陷。</description></item><item><title>XSS vs CSRF</title><link>https://rifewang.github.io/translation/web-security/csrf/xss-vs-csrf/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/csrf/xss-vs-csrf/</guid><description>XSS vs CSRF Link to heading 在本节中，我们将解释 XSS 和 CSRF 之间的区别，并讨论 CSRF token 是否有助于防御 XSS 攻击。
XSS 和 CSRF 之间有啥区别 Link to heading 跨站脚本攻击 XSS 允许攻击者在受害者用户的浏览器中执行任意 JavaScript 。
跨站请求伪造 CSRF 允许攻击者伪造受害用户执行他们不打算执行的操作。
XSS 漏洞的后果通常比 CSRF 漏洞更严重：
CSRF 通常只适用于用户能够执行的操作的子集。通常，许多应用程序都实现 CSRF 防御，但是忽略了暴露的一两个操作。相反，成功的 XSS 攻击通常可以执行用户能够执行的任何操作，而不管该漏洞是在什么功能中产生的。 CSRF 可以被描述为一个“单向”漏洞，因为尽管攻击者可以诱导受害者发出 HTTP 请求，但他们无法从该请求中检索响应。相反，XSS 是“双向”的，因为攻击者注入的脚本可以发出任意请求、读取响应并将数据传输到攻击者选择的外部域。 CSRF token 能否防御 XSS 攻击 Link to heading 一些 XSS 攻击确实可以通过有效使用 CSRF token 来进行防御。假设有一个简单的反射型 XSS 漏洞，其可以被利用如下：
https://insecure-website.com/status?message=&amp;lt;script&amp;gt;/*+Bad+stuff+here...+*/&amp;lt;/script&amp;gt; 现在，假设漏洞函数包含一个 CSRF token :
https://insecure-website.com/status?csrf-token=CIwNZNlR4XbisJF39I8yWnWX9wX4WFoz&amp;amp;message=&amp;lt;script&amp;gt;/*+Bad+stuff+here...+*/&amp;lt;/script&amp;gt; 如果服务器正确地验证了 CSRF token ，并拒绝了没有有效令牌的请求，那么该令牌确实可以防止此 XSS 漏洞的利用。这里的关键点是“跨站脚本”的攻击中涉及到了跨站请求，因此通过防止攻击者伪造跨站请求，该应用程序可防止对 XSS 漏洞的轻度攻击。</description></item><item><title>web 安全之 DOM-based vulnerabilities</title><link>https://rifewang.github.io/translation/web-security/dom-based/dom-based-vulnerabilities/</link><pubDate>Sun, 07 Mar 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/dom-based/dom-based-vulnerabilities/</guid><description>DOM-based vulnerabilities Link to heading 在本节中，我们将描述什么是 DOM ，解释对 DOM 数据的不安全处理是如何引入漏洞的，并建议如何在您的网站上防止基于 DOM 的漏洞。
什么是 DOM Link to heading Document Object Model（DOM）文档对象模型是 web 浏览器对页面上元素的层次表示。网站可以使用 JavaScript 来操作 DOM 的节点和对象，以及它们的属性。DOM 操作本身不是问题，事实上，它也是现代网站中不可或缺的一部分。然而，不安全地处理数据的 JavaScript 可能会引发各种攻击。当网站包含的 JavaScript 接受攻击者可控制的值（称为 source 源）并将其传递给一个危险函数（称为 sink 接收器）时，就会出现基于 DOM 的漏洞。
污染流漏洞 Link to heading 许多基于 DOM 的漏洞可以追溯到客户端代码在处理攻击者可以控制的数据时存在问题。
什么是污染流 Link to heading 要利用或者缓解这些漏洞，首先要熟悉 source 源与 sink 接收器之间的污染流的基本概念。
Source 源是一个 JavaScript 属性，它接受可能由攻击者控制的数据。源的一个示例是 location.search 属性，因为它从 query 字符串中读取输入，这对于攻击者来说比较容易控制。总之，攻击者可以控制的任何属性都是潜在的源。包括引用 URL（ document.referrer ）、用户的 cookies（ document.cookie ）和 web messages 。</description></item><item><title>DOM clobbering</title><link>https://rifewang.github.io/translation/web-security/dom-based/dom-clobbering/</link><pubDate>Sun, 07 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/dom-based/dom-clobbering/</guid><description>DOM clobbering Link to heading 在本节中，我们将描述什么是 DOM clobbing ，演示如何使用 clobbing 技术来利用 DOM 漏洞，并提出防御 DOM clobbing 攻击的方法。
什么是 DOM clobbering Link to heading DOM clobbering 是一种将 HTML 注入页面以操作 DOM 并最终改变页面上 JavaScript 行为的技术。在无法使用 XSS ，但是可以控制页面上 HTML 白名单属性如 id 或 name 时，DOM clobbering 就特别有用。DOM clobbering 最常见的形式是使用 anchor 元素覆盖全局变量，然后该变量将会被应用程序以不安全的方式使用，例如生成动态脚本 URL 。
术语 clobbing 来自以下事实：你正在 “clobbing”（破坏） 一个全局变量或对象属性，并用 DOM 节点或 HTML 集合去覆盖它。例如，可以使用 DOM 对象覆盖其他 JavaScript 对象并利用诸如 submit 这样不安全的名称，去干扰表单真正的 submit() 函数。
如何利用 DOM-clobbering 漏洞 Link to heading 某些 JavaScript 开发者经常会使用以下模式：</description></item><item><title>web 安全之 HTTP Host header attacks</title><link>https://rifewang.github.io/translation/web-security/http-host-header-attacks/http-host-header-attacks/</link><pubDate>Sat, 06 Mar 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/http-host-header-attacks/http-host-header-attacks/</guid><description>HTTP Host header attacks Link to heading 在本节中，我们将讨论错误的配置和有缺陷的业务逻辑如何通过 HTTP Host 头使网站遭受各种攻击。我们将概述识别易受 HTTP Host 头攻击的网站的高级方法，并演示如何利用此方法。最后，我们将提供一些有关如何保护自己网站的一般建议。
什么是 HTTP Host 头 Link to heading 从 HTTP/1.1 开始，HTTP Host 头是一个必需的请求头，其指定了客户端想要访问的域名。例如，当用户访问 https://portswigger.net/web-security 时，浏览器将会发出一个包含 Host 头的请求：
GET /web-security HTTP/1.1 Host: portswigger.net 在某些情况下，例如当请求被中介系统转发时，Host 值可能在到达预期的后端组件之前被更改。我们将在下面更详细地讨论这种场景。
HTTP Host 头的作用是什么 Link to heading HTTP Host 头的作用就是标识客户端想要与哪个后端组件通信。如果请求没有 Host 头或者 Host 格式不正确，则把请求路由到预期的应用程序时会出现问题。
历史上因为每个 IP 地址只会托管单个域名的内容，所以并不存在模糊性。但是如今，由于基于云的解决方案和相关架构的不断增长，使得多个网站和应用程序在同一个 IP 地址访问变得很常见，这种方式也越来越受欢迎，部分原因是 IPv4 地址耗尽。
当多个应用程序通过同一个 IP 地址访问时，通常是以下情况之一。
虚拟主机 Link to heading 一种可能的情况是，一台 web 服务器部署多个网站或应用程序，这可能是同一个所有者拥有多个网站，也有可能是不同网站的所有者部署在同一个共享平台上。这在以前不太常见，但在一些基于云的 SaaS 解决方案中仍然会出现。</description></item><item><title>Exploiting HTTP Host header vulnerabilities</title><link>https://rifewang.github.io/translation/web-security/http-host-header-attacks/exploiting-http-host-header-vulnerabilities/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/http-host-header-attacks/exploiting-http-host-header-vulnerabilities/</guid><description>如何识别和利用 HTTP Host 头漏洞 Link to heading 在本节中，我们将更仔细地了解如何识别网站是否存在 HTTP Host 头漏洞。然后，我们将提供一些示例，说明如何利用此漏洞。
如何使用 HTTP Host 头测试漏洞 Link to heading 要测试网站是否易受 HTTP Host 攻击，你需要一个拦截代理（如 Burp proxy ）和手动测试工具（如 Burp Repeater 和 Burp intruiter ）。
简而言之，你需要能够修改 Host 标头，并且你的请求能够到达目标应用程序。如果是这样，则可以使用此标头来探测应用程序，并观察其对响应的影响。
提供一个任意的 Host 头 Link to heading 在探测 Host 头注入漏洞时，第一步测试是给 Host 头设置任意的、无法识别的域名，然后看看会发生什么。
一些拦截代理直接从 Host 头连接目标 IP 地址，这使得这种测试几乎不可能；对报头所做的任何更改都会导致请求发送到完全不同的 IP 地址。然而，Burp Suite 精确地保持了主机头和目标 IP 地址之间的分离，这种分离允许你提供所需的任意或格式错误的主机头，同时仍然确保将请求发送到预期目标。
有时，即使你提供了一个意外的 Host 头，你仍然可以访问目标网站。这可能有很多原因。例如，服务器有时设置了默认或回退选项，以处理无法识别的域名请求。如果你的目标网站碰巧是默认的，那你就走运了。在这种情况下，你可以开始研究应用程序对 Host 头做了什么，以及这种行为是否可利用。
另一方面，由于 Host 头是网站工作的基本部分，篡改它通常意味着你将无法访问目标应用程序。接收到你的请求的反向代理或负载平衡器可能根本不知道将其转发到何处，从而响应 &amp;ldquo;Invalid Host header&amp;rdquo; 这种错误。如果你的目标很可能是通过 CDN 访问的。在这种情况下，你应该继续尝试下面概述的一些技术。</description></item><item><title>Password reset poisoning</title><link>https://rifewang.github.io/translation/web-security/http-host-header-attacks/password-reset-poisoning/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/http-host-header-attacks/password-reset-poisoning/</guid><description>Password reset poisoning Link to heading 密码重置中毒是一种技术，攻击者可以利用该技术来操纵易受攻击的网站，以生成指向其控制下的域的密码重置链接。这种行为可以用来窃取重置任意用户密码所需的秘密令牌，并最终危害他们的帐户。
密码重置是如何工作的 Link to heading 几乎所有需要登录的网站都实现了允许用户在忘记密码时重置密码的功能。实现这个功能有好几种方法，其中一个最常见的方法是：
用户输入用户名或电子邮件地址，然后提交密码重置请求。 网站检查该用户是否存在，然后生成一个临时的、唯一的、高熵的 token 令牌，并在后端将该令牌与用户的帐户相关联。 网站向用户发送一封包含重置密码链接的电子邮件。用户的 token 令牌作为 query 参数包含在相应的 URL 中，如 https://normal-website.com/reset?token=0a1b2c3d4e5f6g7h8i9j。 当用户访问此 URL 时，网站会检查所提供的 token 令牌是否有效，并使用它来确定要重置的帐户。如果一切正常，用户就可以设置新密码了。最后，token 令牌被销毁。 与其他一些方法相比，这个过程足够简单并且相对安全。然而，它的安全性依赖于这样一个前提：只有目标用户才能访问他们的电子邮件收件箱，从而使用他们的 token 令牌。而密码重置中毒就是一种窃取此 token 令牌以更改其他用户密码的方法。
如何构造一个密码重置中毒攻击 Link to heading 如果发送给用户的 URL 是基于可控制的输入（例如 Host 头）动态生成的，则可以构造如下所示的密码重置中毒攻击：
攻击者根据需要获取受害者的电子邮件地址或用户名，并代表受害者提交密码重置请求，但是这个请求被修改了 Host 头，以指向他们控制的域。我们假设使用的是 evil-user.net 。 受害者收到了网站发送的真实的密码重置电子邮件，其中包含一个重置密码的链接，以及与他们的帐户相关联的 token 令牌。但是，URL 中的域名指向了攻击者的服务器：https://evil-user.net/reset?token=0a1b2c3d4e5f6g7h8i9j 。 如果受害者点击了此链接，则密码重置的 token 令牌将被传递到攻击者的服务器。 攻击者现在可以访问网站的真实 URL ，并使用盗取的受害者的 token 令牌，将用户的密码重置为自己的密码，然后就可以登录到用户的帐户了。 在真正的攻击中，攻击者可能会伪造一个假的警告通知来提高受害者点击链接的概率。
即使不能控制密码重置的链接，有时也可以使用 Host 头将 HTML 注入到敏感的电子邮件中。请注意，电子邮件客户端通常不执行 JavaScript ，但其他 HTML 注入技术如悬挂标记攻击可能仍然适用。</description></item><item><title>web 安全之 Clickjacking</title><link>https://rifewang.github.io/translation/web-security/clickjacking/clickjacking/</link><pubDate>Fri, 05 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/clickjacking/clickjacking/</guid><description>Clickjacking ( UI redressing ) Link to heading 在本节中，我们将解释什么是 clickjacking 点击劫持，并描述常见的点击劫持攻击示例，以及讨论如何防御这些攻击。
什么是点击劫持 Link to heading 点击劫持是一种基于界面的攻击，通过诱导用户点击钓鱼网站中的被隐藏了的可操作的危险内容。
例如：某个用户被诱导访问了一个钓鱼网站（可能是点击了电子邮件中的链接），然后点击了一个赢取大奖的按钮。实际情况则是，攻击者在这个赢取大奖的按钮下面隐藏了另一个网站上向其他账户进行支付的按钮，而结果就是用户被诱骗进行了支付。这就是一个点击劫持攻击的例子。这项技术实际上就是通过 iframe 合并两个页面，真实操作的页面被隐藏，而诱骗用户点击的页面则显示出来。点击劫持攻击与 CSRF 攻击的不同之处在于，点击劫持需要用户执行某种操作，比如点击按钮，而 CSRF 则是在用户不知情或者没有输入的情况下伪造整个请求。
针对 CSRF 攻击的防御措施通常是使用 CSRF token（针对特定会话、一次性使用的随机数）。而点击劫持无法则通过 CSRF token 缓解攻击，因为目标会话是在真实网站加载的内容中建立的，并且所有请求均在域内发生。CSRF token 也会被放入请求中，并作为正常行为的一部分传递给服务器，与普通会话相比，差异就在于该过程发生在隐藏的 iframe 中。
如何构造一个基本的点击劫持攻击 Link to heading 点击劫持攻击使用 CSS 创建和操作图层。攻击者将目标网站通过 iframe 嵌入并隐藏。使用样式标签和参数的示例如下：
&amp;lt;head&amp;gt; &amp;lt;style&amp;gt; #target_website { position:relative; width:128px; height:128px; opacity:0.00001; z-index:2; } #decoy_website { position:absolute; width:300px; height:400px; z-index:1; } &amp;lt;/style&amp;gt; &amp;lt;/head&amp;gt; ... &amp;lt;body&amp;gt; &amp;lt;div id=&amp;#34;decoy_website&amp;#34;&amp;gt; ...decoy web content here.</description></item><item><title>web 安全之 HTTP request smuggling</title><link>https://rifewang.github.io/translation/web-security/request-smuggling/http-request-smuggling/</link><pubDate>Thu, 04 Mar 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/request-smuggling/http-request-smuggling/</guid><description>HTTP request smuggling Link to heading 在本节中，我们将解释什么是 HTTP 请求走私，并描述常见的请求走私漏洞是如何产生的。
什么是 HTTP 请求走私 Link to heading HTTP 请求走私是一种干扰网站处理多个 HTTP 请求序列的技术。请求走私漏洞危害很大，它使攻击者可以绕过安全控制，未经授权访问敏感数据并直接危害其他应用程序用户。
HTTP 请求走私到底发生了什么 Link to heading 现在的应用架构中经常会使用诸如负载均衡、反向代理、网关等服务，这些服务在链路上起到了一个转发请求给后端服务器的作用，因为位置位于后端服务器的前面，所以本文把他们称为前端服务器。
当前端服务器（转发服务）将 HTTP 请求转发给后端服务器时，它通常会通过与后端服务器之间的同一个网络连接发送多个请求，因为这样做更加高效。协议非常简单：HTTP 请求被一个接一个地发送，接受请求的服务器则解析 HTTP 请求头以确定一个请求的结束位置和下一个请求的开始位置，如下图所示： 在这种情况下，前端服务器（转发服务）与后端系统必须就请求的边界达成一致。否则，攻击者可能会发送一个模棱两可的请求，该请求被前端服务器（转发服务）与后端系统以不同的方式解析：
如上图所示，攻击者使上一个请求的一部分被后端服务器解析为下一个请求的开始，这时就会干扰应用程序处理该请求的方式。这就是请求走私攻击，其可能会造成毁灭性的后果。
HTTP 请求走私漏洞是怎么产生的 Link to heading 绝大多数 HTTP 请求走私漏洞的出现是因为 HTTP 规范提供了两种不同的方法来指定请求的结束位置：Content-Length 头和 Transfer-Encoding 头。
Content-Length 头很简单，直接以字节为单位指定消息体的长度。例如：
POST /search HTTP/1.1 Host: normal-website.com Content-Type: application/x-www-form-urlencoded Content-Length: 11 q=smuggling Transfer-Encoding 头则可以声明消息体使用了 chunked 编码，就是消息体被拆分成了一个或多个分块传输，每个分块的开头是当前分块大小（以十六进制表示），后面紧跟着 \r\n，然后是分块内容，后面也是 \r\n。消息的终止分块也是同样的格式，只是其长度为零。例如：
POST /search HTTP/1.1 Host: normal-website.com Content-Type: application/x-www-form-urlencoded Transfer-Encoding: chunked b q=smuggling 0 由于 HTTP 规范提供了两种不同的方法来指定 HTTP 消息的长度，因此单个消息中完全可以同时使用这两种方法，从而使它们相互冲突。HTTP 规范为了避免这种歧义，其声明如果 Content-Length 和 Transfer-Encoding 同时存在，则 Content-Length 应该被忽略。当只有一个服务运行时，这种歧义似乎可以避免，但是当多个服务被连接在一起时，这种歧义就无法避免了。在这种情况下，出现问题有两个原因：</description></item><item><title>Exploiting request smuggling</title><link>https://rifewang.github.io/translation/web-security/request-smuggling/exploiting-request-smuggling/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/request-smuggling/exploiting-request-smuggling/</guid><description>利用 HTTP 请求走私漏洞 Link to heading 在本节中，我们将描述 HTTP 请求走私漏洞的几种利用方法，这也取决于应用程序的预期功能和其他行为。
利用 HTTP 请求走私漏洞绕过前端服务器（转发服务）安全控制 Link to heading 在某些应用程序中，前端服务器（转发服务）不仅用来转发请求，也用来实现了一些安全控制，以决定单个请求能否被转发到后端处理，而后端服务认为接受到的所有请求都已经通过了安全验证。
假设，某个应用程序使用前端服务器（转发服务）来做访问控制，只有当用户被授权访问的请求才会被转发给后端服务器，后端服务器接受的所有请求都无需进一步检查。在这种情况下，可以使用 HTTP 请求走私漏洞绕过访问控制，将请求走私到后端服务器。
假设当前用户可以访问 /home ，但不能访问 /admin 。他们可以使用以下请求走私攻击绕过此限制：
前端服务器（转发服务）将其视为一个请求，然后进行访问验证，由于用户拥有访问 /home 的权限，因此把请求转发给后端服务器。然而，后端服务器则将其视为 /home 和 /admin 两个单独的请求，并且认为请求都通过了权限验证，此时 /admin 的访问控制实际上就被绕过了。
前端服务器（转发服务）对请求重写 Link to heading 在许多应用程序中，请求被转发给后端服务之前会进行一些重写，通常是添加一些额外的请求头之类的。例如，转发请求重写可能：
终止 TLS 连接并添加一些描述使用的协议和密钥之类的头。 添加 X-Forwarded-For 头用来标记用户的 IP 地址。 根据用户的会话令牌确定用户 ID ，并添加用于标识用户的头。 添加一些其他攻击感兴趣的敏感信息。 在某些情况下，如果你走私的请求缺少一些前端服务器（转发服务）添加的头，那么后端服务可能不会正常处理，从而导致走私请求无法达到预期的效果。
通常有一些简单的方法可以准确地得知前端服务器（转发服务）是如何重写请求的。为此，需要执行以下步骤：
找到一个将请求参数的值反映到应用程序响应中的 POST 请求。 随机排列参数，以使反映的参数出现在消息体的最后。 将这个请求走私到后端服务器，然后直接发送一个要显示其重写形式的普通请求。 假设应用程序有个登录的功能，其会反映 email 参数：
POST /login HTTP/1.1 Host: vulnerable-website.com Content-Type: application/x-www-form-urlencoded Content-Length: 28 email=wiener@normal-user.</description></item><item><title>Finding request smuggling</title><link>https://rifewang.github.io/translation/web-security/request-smuggling/finding-request-smuggling/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/request-smuggling/finding-request-smuggling/</guid><description>查找 HTTP 请求走私漏洞 Link to heading 在本节中，我们将介绍用于查找 HTTP 请求走私漏洞的不同技术。
计时技术 Link to heading 检测 HTTP 请求走私漏洞的最普遍有效的方法就是计时技术。发送请求，如果存在漏洞，则应用程序的响应会出现时间延迟。
使用计时技术查找 CL.TE 漏洞 Link to heading 如果应用存在 CL.TE 漏洞，那么发送如下请求通常会导致时间延迟：
前端服务器（转发服务）使用 Content-Length 认为消息体只有 4 个字节，即 1\r\nA，因此后面的 X 被忽略了，然后把这个请求转发给后端。而后端服务使用 Transfer-Encoding 则会一直等待终止分块 0\r\n 。这就会导致明显的响应延迟。
使用计时技术查找 TE.CL 漏洞 Link to heading 如果应用存在 TE.CL 漏洞，那么发送如下请求通常会导致时间延迟：
前端服务器（转发服务）使用 Transfer-Encoding，由于第一个分块就是 0\r\n 终止分块，因此后面的 X 直接被忽略了，然后把这个请求转发给后端。而后端服务使用 Content-Length 则会一直等到后续 6 个字节的内容。这就会导致明显的延迟。
注意：如果应用程序易受 CL.TE 漏洞的攻击，则基于时间的 TE.CL 漏洞测试可能会干扰其他应用程序用户。因此，为了隐蔽并尽量减少干扰，你应该先进行 CL.TE 测试，只有在失败了之后再进行 TE.CL 测试。
使用差异响应确认 HTTP 请求走私漏洞 Link to heading 当检测到可能的请求走私漏洞时，可以通过利用该漏洞触发应用程序响应内容的差异来获取该漏洞进一步的证据。这包括连续向应用程序发送两个请求：</description></item><item><title>web 安全之 OS command injection</title><link>https://rifewang.github.io/translation/web-security/command-injection/os-command-injection/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/command-injection/os-command-injection/</guid><description>OS command injection Link to heading 在本节中，我们将解释什么是操作系统命令注入，描述如何检测和利用此漏洞，为不同的操作系统阐明一些有用的命令和技术，并总结如何防止操作系统命令注入。
什么是操作系统命令注入 Link to heading OS 命令注入（也称为 shell 注入）是一个 web 安全漏洞，它允许攻击者在运行应用程序的服务器上执行任意的操作系统命令，这通常会对应用程序及其所有数据造成严重危害。并且，攻击者也常常利用此漏洞危害基础设施中的其他部分，利用信任关系攻击组织内的其他系统。
执行任意命令 Link to heading 假设某个购物应用程序允许用户查看某个商品在特定商店中是否有库存，此信息可以通过以下 URL 获取：
https://insecure-website.com/stockStatus?productID=381&amp;amp;storeID=29 为了提供返回信息，应用程序必须查询各种遗留系统。由于历史原因，此功能通过调用 shell 命令并传递参数来实现如下：
stockreport.pl 381 29 此命令输出特定商店中某个商品的库存信息，并将其返回给用户。
由于应用程序没有对 OS 命令注入进行防御，那么攻击者可以提交类似以下输入来执行任意命令：
&amp;amp; echo aiwefwlguh &amp;amp; 如果这个输入被当作 productID 参数，那么应用程序执行的命令就是：
stockreport.pl &amp;amp; echo aiwefwlguh &amp;amp; 29 echo 命令就是让提供的字符串在输出中显示的作用，其是测试某些 OS 命令注入的有效方法。&amp;amp; 符号就是一个 shell 命令分隔符，因此上例实际执行的是一个接一个的三个单独的命令。因此，返回给用户的输出为：
Error - productID was not provided aiwefwlguh 29: command not found 这三行输出表明：
原来的 stockreport.pl 命令由于没有收到预期的参数，因此返回错误信息。 注入的 echo 命令执行成功。 原始的参数 29 被当成了命令执行，也导致了异常。 将命令分隔符 &amp;amp; 放在注入命令之后通常是有用的，因为它会将注入的命令与注入点后面的命令分开，这减少了随后发生的事情将阻止注入命令执行的可能性。</description></item><item><title>web 安全之 Server-side request forgery</title><link>https://rifewang.github.io/translation/web-security/ssrf/ssrf/</link><pubDate>Mon, 01 Mar 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/ssrf/ssrf/</guid><description>Server-side request forgery (SSRF) Link to heading 在本节中，我们将解释 server-side request forgery（服务端请求伪造）是什么，并描述一些常见的示例，以及解释如何发现和利用各种 SSRF 漏洞。
SSRF 是什么 Link to heading SSRF 服务端请求伪造是一个 web 漏洞，它允许攻击者诱导服务端程序向攻击者选择的任何地址发起 HTTP 请求。
在典型的 SSRF 示例中，攻击者可能会使服务端建立一个到服务端自身、或组织基础架构中的其它基于 web 的服务、或外部第三方系统的连接。
SSRF 攻击的影响 Link to heading 成功的 SSRF 攻击通常会导致未经授权的操作或对组织内部数据的访问，无论是在易受攻击的应用程序本身，还是应用程序可以通信的其它后端系统。在某些情况下，SSRF 漏洞可能允许攻击者执行任意的命令。
利用 SSRF 漏洞可能可以操作服务端应用程序使其向与之连接的外部第三方系统发起恶意请求，这将导致潜在的法律责任和声誉受损。
常见的 SSRF 攻击 Link to heading SSRF 攻击通常利用服务端应用程序的信任关系发起攻击并执行未经授权的操作。这种信任关系可能包括：对服务端自身的信任，或同组织内其它后端系统的信任。
SSRF 攻击服务端自身 Link to heading 在针对服务端本身的 SSRF 攻击中，攻击者诱导应用程序向其自身发出 HTTP 请求，这通常需要提供一个主机名是 127.0.0.1 或者 localhost 的 URL 。
例如，假设某个购物应用程序，其允许用户查看某个商品在特定商店中是否有库存。为了提供库存信息，应用程序需要通过 REST API 查询其他后端服务，而其他后端服务的 URL 地址直接包含在前端 HTTP 请求中。因此，当用户查看商品的库存状态时，浏览器可能发出如下请求：</description></item><item><title>Blind SSRF</title><link>https://rifewang.github.io/translation/web-security/ssrf/blind-ssrf/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/ssrf/blind-ssrf/</guid><description>Blind SSRF Link to heading 在本节中，我们将解释什么是不可见的服务端请求伪造，并描述一些常见的不可见 SSRF 示例，以及解释如何发现和利用不可见 SSRF 漏洞。
什么是不可见 SSRF Link to heading 不可见 SSRF 漏洞是指，可以诱导应用程序向提供的 URL 发出后端 HTTP 请求，但来自后端请求的响应没有在应用程序的前端响应中返回。
不可见 SSRF 漏洞的影响 Link to heading 不可见 SSRF 漏洞的影响往往低于完全可见的 SSRF 漏洞，因为其单向性，虽然在某些情况下，可以利用它们从后端系统检索敏感数据，但不能轻易地利用它们来实现完整的远程代码执行。
如何发现和利用不可见 SSRF 漏洞 Link to heading 检测不可见 SSRF 漏洞最可靠的方法是使用 out-of-band（OAST）带外技术。这包括尝试触发对你控制的外部系统的 HTTP 请求，并监视与该系统的网络交互。
使用 OAST 技术最简单有效的方式是使用 Burp Collaborator (付费软件)。你可以使用 Burp Collaborator client 生成唯一的域名，将这个域名以有效负载的形式发送到检测漏洞的应用程序，并监视与这个域名的任何交互，如果观察到来自应用程序传入的 HTTP 请求，则说明应用程序存在 SSRF 漏洞。
注意：在测试 SSRF 漏洞时，通常会观察到所提供域名的 DNS 查找，但是却没有后续的 HTTP 请求。这通常是应用程序视图向该域名发出 HTTP 请求，这导致了初始的 DNS 查找，但实际的 HTTP 请求被网络拦截了。基础设施允许出站的 DNS 流量是相对常见的，因为出于很多目的需要，但是会阻止到意外目的地的 HTTP 连接。</description></item><item><title>web 安全之 Directory traversal</title><link>https://rifewang.github.io/translation/web-security/directory-traversal/directory-traversal/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/directory-traversal/directory-traversal/</guid><description>Directory traversal - 目录遍历 Link to heading 在本节中，我们将介绍什么是目录遍历，描述如何执行路径遍历攻击和绕过常见障碍，并阐明如何防止路径遍历漏洞。
什么是目录遍历？ Link to heading 目录遍历（也称为文件路径遍历）是一个 web 安全漏洞，此漏洞使攻击者能够读取运行应用程序的服务器上的任意文件。这可能包括应用程序代码和数据、后端系统的凭据以及操作系统相关敏感文件。在某些情况下，攻击者可能能够对服务器上的任意文件进行写入，从而允许他们修改应用程序数据或行为，并最终完全控制服务器。
通过目录遍历读取任意文件 Link to heading 假设某个应用程序通过如下 HTML 加载图像：
&amp;lt;img src=&amp;#34;/loadImage?filename=218.png&amp;#34;&amp;gt; 这个 loadImage URL 通过 filename 文件名参数来返回指定文件的内容，假设图像本身存储在路径为 /var/www/images/ 的磁盘上。应用程序基于此基准路径与请求的 filename 文件名返回如下路径的图像：
/var/www/images/218.png 如果该应用程序没有针对目录遍历攻击采取任何防御措施，那么攻击者可以请求类似如下 URL 从服务器的文件系统中检索任意文件：
https://insecure-website.com/loadImage?filename=../../../etc/passwd 这将导致如下路径的文件被返回：
/var/www/images/../../../etc/passwd ../ 表示上级目录，因此这个文件其实就是：
/etc/passwd 在 Unix 操作系统上，这个文件是一个内容为该服务器上注册用户详细信息的标准文件。
在 Windows 系统上，..\ 和 ../ 的作用相同，都表示上级目录，因此检索标准操作系统文件可以通过如下方式：
https://insecure-website.com/loadImage?filename=..\..\..\windows\win.ini 利用文件路径遍历漏洞的常见障碍 Link to heading 许多将用户输入放入文件路径的应用程序实现了某种应对路径遍历攻击的防御措施，然而这些措施却通常可以被规避。
如果应用程序从用户输入的 filename 中剥离或阻止 ..\ 目录遍历序列，那么也可以使用各种技巧绕过防御。
你可以使用从系统根目录开始的绝对路径，例如 filename=/etc/passwd 这样直接引用文件而不使用任何 ..\ 形式的遍历序列。</description></item><item><title>web 安全之 CORS</title><link>https://rifewang.github.io/translation/web-security/cors/cors/</link><pubDate>Sun, 28 Feb 2021 01:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/cors/cors/</guid><description>Cross-origin resource sharing (CORS) Link to heading 在本节中，我们将解释什么是跨域资源共享（CORS），并描述一些基于 CORS 的常见攻击示例，以及讨论如何防御这些攻击。
CORS（跨域资源共享）是什么？ Link to heading CORS（跨域资源共享）是一种浏览器机制，它允许对位于当前访问域之外的资源进行受控访问。它扩展并增加了同源策略的灵活性。然而，如果一个网站的 CORS 策略配置和实现不当，它也可能导致基于跨域的攻击。CORS 不是针对跨源攻击（例如跨站请求伪造 CSRF）的保护。
Same-origin policy（同源策略） Link to heading 同源策略是一种限制性的跨域规范，它限制了网站与源域之外资源交互的能力。同源策略是多年前定义的，用于应对潜在的恶意跨域交互，例如一个网站从另一个网站窃取私人数据。它通常允许域向其他域发出请求，但不允许访问响应。
更多内容请参考 Same-origin-policy 。
同源策略的放宽 Link to heading 同源策略具有很大的限制性，因此人们设计了很多方法去规避这些限制。许多网站与子域或第三方网站的交互方式要求完全的跨域访问。使用跨域资源共享（CORS）可以有控制地放宽同源策略。
CORS 协议使用一组 HTTP header 来定义可信的 web 域和相关属性，例如是否允许通过身份验证的访问。浏览器和它试图访问的跨域网站之间进行这些 header 的交换。
更多内容请参考 CORS and the Access-Control-Allow-Origin response header 。
CORS 配置不当引发的漏洞 Link to heading 现在许多网站使用 CORS 来允许来自子域和可信的第三方的访问。他们对 CORS 的实现可能包含有错误或过于放宽，这可能导致可利用的漏洞。
服务端 ACAO 直接返回客户端的 Origin Link to heading 有些应用程序需要允许很多其它域的访问。维护一个允许域的列表需要付出持续的努力，任何差错都有可能造成破坏。因此，应用程序可能使用一些更加简单的方法来达到最终目的。
一种方法是从请求头中读取 Origin，然后将其作为 Access-Control-Allow-Origin 响应头返回。例如，应用程序接受了以下请求：</description></item><item><title>CORS 和 Access-Control-Allow-Origin 响应头</title><link>https://rifewang.github.io/translation/web-security/cors/access-control-allow-origin/</link><pubDate>Sun, 28 Feb 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/cors/access-control-allow-origin/</guid><description>CORS 和 Access-Control-Allow-Origin 响应头 Link to heading 在本节中，我们将解释有关 CORS 的 Access-Control-Allow-Origin 响应头，以及后者如何构成 CORS 实现的一部分。
CORS 通过使用一组 HTTP 头部提供了同源策略的可控制放宽，浏览器允许访问基于这些头部的跨域请求的响应。
什么是 Access-Control-Allow-Origin 响应头？ Link to heading Access-Control-Allow-Origin 响应头标识了跨域请求允许的请求来源，浏览器会将 Access-Control-Allow-Origin 与请求网站 origin 进行比较，如果两者匹配则允许访问响应。
实现简单的 CORS Link to heading CORS 规范规定了 web 服务器和浏览器之间交换的头内容，其中 Access-Control-Allow-Origin 是最重要的。当网站发起跨域资源请求时，浏览器将会自动添加 Origin 头，随后服务器返回 Access-Control-Allow-Origin 响应头。
例如，origin 为 normal-website.com 的网站发起了如下跨域请求：
GET /data HTTP/1.1 Host: robust-website.com Origin : https://normal-website.com 服务器响应：
HTTP/1.1 200 OK ... Access-Control-Allow-Origin: https://normal-website.com 浏览器将会允许 normal-website.com 网站代码访问响应，因为 Access-Control-Allow-Origin 与 Origin 匹配。</description></item><item><title>Same-origin policy (SOP)</title><link>https://rifewang.github.io/translation/web-security/cors/same-origin-policy/</link><pubDate>Sun, 28 Feb 2021 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/translation/web-security/cors/same-origin-policy/</guid><description>Same-origin policy (SOP) - 同源策略 Link to heading 在本节中，我们将解释什么是同源策略以及它是如何实现的。
什么是同源策略？ Link to heading 同源策略是一种旨在防止网站互相攻击的 web 浏览器的安全机制。
同源策略限制一个源上的脚本访问另一个源的数据。
Origin 源由三个部分组成：schema、domain、port ，所谓的同源就是要求这三个部分全部相同。 例如下面这个 URL：
http://normal-website.com/example/example.html 其 schema 是 http，domain 是 normal-website.com，port 是 80 。下表显示了如果上述 URL 中的内容尝试访问其它源将会是什么情况：
访问的 URL 是否可以访问 http://normal-website.com/example/ 是，同源 http://normal-website.com/example2/ 是，同源 https://normal-website.com/example/ 否: scheme 和 port 都不同 http://en.normal-website.com/example/ 否: domain 不同 http://www.normal-website.com/example/ 否: domain 不同 http://normal-website.com:8080/example/ 否: port 不同* *IE 浏览器将会允许访问，因为 IE 浏览器在应用同源策略时不考虑端口号。
为什么同源策略是必要的？ Link to heading 当浏览器从一个源发送 HTTP 请求到另一个源时，与另一个源相关的任何 cookie （包括身份验证会话cookie）也将会作为请求的一部分一起发送。这意味着响应将在用户会话中返回，并包含此特定用户的相关数据。如果没有同源策略，如果你访问了一个恶意网站，它将能够读取你 GMail 中的电子邮件、Facebook 上的私人消息等。</description></item><item><title>解读 MySQL Client/Server Protocol: Connection &amp; Replication</title><link>https://rifewang.github.io/posts/mysql/protocol-connectionreplication/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/mysql/protocol-connectionreplication/</guid><description>解读 MySQL Client/Server Protocol: Connection &amp;amp; Replication Link to heading MySQL 客户端与服务器之间的通信基于特定的 TCP 协议，本文将会详解其中的 Connection 和 Replication 部分，这两个部分分别对应的是客户端与服务器建立连接、完成认证鉴权，以及客户端注册成为一个 slave 并获取 master 的 binlog 日志。
Connetcion Phase Link to heading MySQL 客户端想要与服务器进行通信，第一步就是需要成功建立连接，整个过程如下图所示：
client 发起一个 TCP 连接。 server 响应一个 Initial Handshake Packet（初始化握手包），内容会包含一个默认的认证方式。 这一步是可选的，双方建立 SSL 加密连接。 client 回应 Handshake Response Packet，内容需要包括用户名和按照指定方式进行加密后的密码数据。 server 响应 OK_Packet 确认认证成功，或者 ERR_Packet 表示认证失败并关闭连接。 Packet Link to heading 一个 Packet 其实就是一个 TCP 包，所有包都有一个最基本的结构：
如上图所示，所有包都可以看作由 header 和 body 两部分构成：第一部分 header 总共有 4 个字节，3 个字节用来标识 body 即 payload 的大小，1 个字节记录 sequence ID；第二部分 body 就是 payload 实际的负载数据。</description></item><item><title>同步 MySQL 数据至 Elasticsearch/Redis/MQ 等的五种方式</title><link>https://rifewang.github.io/posts/mysql/sync-data-from-mysql/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/mysql/sync-data-from-mysql/</guid><description>同步 MySQL 数据至 Elasticsearch/Redis/MQ 等的五种方式 Link to heading 在实际应用中，我们经常需要把 MySQL 的数据同步至其它数据源，也就是在对 MySQL 的数据进行了新增、修改、删除等操作后，把该数据相关的业务逻辑变更也应用到其它数据源，例如：
MySQL -&amp;gt; Elasticsearch ，同步 ES 的索引 MySQL -&amp;gt; Redis ，刷新缓存 MySQL -&amp;gt; MQ (如 Kafka 等) ，投递消息 本文总结了五种数据同步的方式。
1. 业务层同步 Link to heading 由于对 MySQL 数据的操作也是在业务层完成的，所以在业务层同步操作另外的数据源也是很自然的，比较常见的做法就是在 ORM 的 hooks 钩子里编写相关同步代码。
这种方式的缺点是，当服务越来越多时，同步的部分可能会过于分散从而导致难以更新迭代，例如对 ES 索引进行不兼容迁移时就可能会牵一发而动全身。
2. 中间件同步 Link to heading 当应用架构演变为微服务时，各个服务里可能不再直接调用 MySQL ，而是通过一层 middleware 中间件，这时候就可以在中间件操作 MySQL 的同时同步其它数据源。
这种方式需要中间件去适配，具有一定复杂度。
3. 定时任务根据 updated_at 字段同步 Link to heading 在 MySQL 的表结构里设置特殊的字段，如 updated_at（数据的更新时间），根据此字段，由定时任务去查询实际变更的数据，从而实现数据的增量更新。</description></item><item><title>Elasticsearch 分布式搜索的运行机制</title><link>https://rifewang.github.io/posts/elasticsearch/es-distribute-search-steps/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-distribute-search-steps/</guid><description>Elasticsearch 分布式搜索的运行机制 Link to heading ES 有两种 search_type 即搜索类型：
query_then_fetch （默认） dfs_query_then_fetch query_then_fetch Link to heading 用户发起搜索，请求到集群中的某个节点。 query 会被发送到所有相关的 shard 分片上。 每个 shard 分片独立执行 query 搜索文档并进行排序分页等，打分时使用的是分片本身的 Local Term/Document 频率。 分片的 query 结果（只有元数据，例如 _id 和 _score）返回给请求节点。 请求节点对所有分片的 query 结果进行汇总，然后根据打分排序和分页，最后选择出搜索结果文档（也只有元数据）。 根据元数据去对应的 shard 分片拉取存储在磁盘上的文档的详细数据。 得到详细的文档数据，组成搜索结果，将结果返回给用户。 缺点：由于每个分片独立使用自身的而不是全局的 Term/Document 频率进行相关度打分，当数据分布不均匀时可能会造成打分偏差，从而影响最终搜索结果的相关性。
dfs_query_then_fetch Link to heading dfs_query_then_fetch 与 query_then_fetch 的运行机制非常类似，但是有两点不同。
用户发起搜索，请求到集群中的某个节点。 预查询每个分片，得到全局的 Global Term/Document 频率。 query 会被发送到所有相关的 shard 分片上。 每个 shard 分片独立执行 query 搜索文档并进行排序分页等，打分时使用的是分片本身的 Global Term/Document 频率。 分片的 query 结果（只有元数据，例如 _id 和 _score）返回给请求节点。 请求节点对所有分片的 query 结果进行汇总，然后根据打分排序和分页，最后选择出搜索结果文档（也只有元数据）。 根据元数据去对应的 shard 分片拉取存储在磁盘上的文档的详细数据。 得到详细的文档数据，组成搜索结果，将结果返回给用户。 缺点：太耗费资源，一般还是不建议使用。</description></item><item><title>Elasticsearch Search Template</title><link>https://rifewang.github.io/posts/elasticsearch/es-search-template/</link><pubDate>Mon, 16 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-search-template/</guid><description>Elasticsearch Search Template Link to heading 所谓 search template 搜索模板其实就是：
预先定义好查询语句 DSL 的结构并预留参数 搜索的时再传入参数值 渲染出完整的 DSL ，最后进行搜索 使用搜索模板可以将 DSL 从应用程序中解耦出来，并且可以更加灵活的更改查询语句。
例如：
GET _search/template { &amp;#34;source&amp;#34; : { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34; : { &amp;#34;{{my_field}}&amp;#34; : &amp;#34;{{my_value}}&amp;#34; } } }, &amp;#34;params&amp;#34; : { &amp;#34;my_field&amp;#34; : &amp;#34;message&amp;#34;, &amp;#34;my_value&amp;#34; : &amp;#34;foo&amp;#34; } } 构造出来的 DSL 就是：
{ &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;message&amp;#34;: &amp;#34;foo&amp;#34; } } } 在模板中通过 {{ }} 的方式预留参数，然后查询时再指定对应的参数值，最后填充成具体的查询语句进行搜索。
搜索模板 API Link to heading 为了实现搜索模板和查询分离，我们首先需要单独保存和管理搜索模板。</description></item><item><title>构造请求日志分析系统</title><link>https://rifewang.github.io/posts/elasticsearch/log-analyzer-system/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/log-analyzer-system/</guid><description>构造请求日志分析系统 Link to heading 请求日志记录哪些数据 Link to heading time_local : 请求的时间 remote_addr : 客户端的 IP 地址 request_method : 请求方法 request_schema : 请求协议，常见的 http 和 https request_host : 请求的域名 request_path : 请求的 path 路径 request_query : 请求的 query 参数 request_size : 请求的大小 referer : 请求来源地址，假设你在 a.com 网站下贴了 b.com 的链接，那么当用户从 a.com 点击访问 b.com 的时候，referer 记录的就是 a.com ，这个是浏览器的行为 user_agent : 客户端浏览器相关信息 status : 请求的响应状态 request_time : 请求的耗时 bytes_sent : 响应的大小 很多时候我们会使用负载网关去代理转发请求给实际的后端服务，这时候请求日志还会包括以下数据：
upstream_host : 代理转发的 host upstream_addr : 代理转发的 IP 地址 upstream_url : 代理转发给服务的 url upstream_status : 上游服务返回的 status proxy_time : 代理转发过程中的耗时 数据衍生 Link to heading 客户端 IP 地址可以衍生出以下数据：</description></item><item><title>Elasticsearch 自定义打分 Function score query</title><link>https://rifewang.github.io/posts/elasticsearch/es-function-score-query/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-function-score-query/</guid><description>Elasticsearch 自定义打分 Function score query Link to heading Elasticsearch 会为 query 的每个文档计算一个相关度得分 score ，并默认按照 score 从高到低的顺序返回搜索结果。 在很多场景下，我们不仅需要搜索到匹配的结果，还需要能够按照某种方式对搜索结果重新打分排序。例如：
搜索具有某个关键词的文档，同时考虑到文档的时效性进行综合排序。 搜索某个旅游景点附近的酒店，同时根据距离远近和价格等因素综合排序。 搜索标题包含 elasticsearch 的文章，同时根据浏览次数和点赞数进行综合排序。 Function score query 就可以让我们实现对最终 score 的自定义打分。
score 自定义打分过程 Link to heading 为了行文方便，本文把 ES 对 query 匹配的文档进行打分得到的 score 记为 query_score ，而最终搜索结果的 score 记为 result_score ，显然，一般情况下（也就是不使用自定义打分时），result_score 就是 query_score 。
那么当我们使用了自定义打分之后呢？最终结果的 score 即 result_score 的计算过程如下：
跟原来一样执行 query 并且得到原来的 query_score 。 执行设置的自定义打分函数，并为每个文档得到一个新的分数，本文记为 func_score 。 最终结果的分数 result_score 等于 query_score 与 func_score 按某种方式计算的结果（默认是相乘）。 例如，搜索标题包含 elasticsearch 的文档。</description></item><item><title>Logstash 入门</title><link>https://rifewang.github.io/posts/elasticsearch/logstash/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/logstash/</guid><description>Logstash 入门 Link to heading Logstash 是什么 Link to heading Logstash 就是一个开源的数据流工具，它会做三件事：
从数据源拉取数据 对数据进行过滤、转换等处理 将处理后的数据写入目标地 例如：
监听某个目录下的日志文件，读取文件内容，处理数据，写入 influxdb 。 从 kafka 中消费消息，处理数据，写入 elasticsearch 。 为什么要用 Logstash ？ Link to heading 方便省事。
假设你需要从 kafka 中消费数据，然后写入 elasticsearch ，如果自己编码，你得去对接 kafka 和 elasticsearch 的 API 吧，如果你用 Logstash ，这部分就不用自己去实现了，因为 Logstash 已经为你封装了对应的 plugin 插件，你只需要写一个配置文件形如：
input { kafka { # kafka consumer 配置 } } filter { # 数据处理配置 } output { elasticsearch { # elasticsearch 输出配置 } } 然后运行 logstash 就可以了。</description></item><item><title>又拍图片管家亿级图像之搜图系统的两代演进及底层原理</title><link>https://rifewang.github.io/posts/engineering/image-search-total/</link><pubDate>Thu, 04 Jun 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/engineering/image-search-total/</guid><description>又拍图片管家亿级图像之搜图系统的两代演进及底层原理 Link to heading 前言 Link to heading 又拍图片管家当前服务了千万级用户，管理了百亿级图片。当用户的图库变得越来越庞大时，业务上急切的需要一种方案能够快速定位图像，即直接输入图像，然后根据输入的图像内容来找到图库中的原图及相似图，而以图搜图服务就是为了解决这个问题。
本人于在职期间独立负责并实施了整个以图搜图系统从技术调研、到设计验证、以及最后工程实现的全过程。而整个以图搜图服务也是经历了两次的整体演进：从 2019 年初开始第一次技术调研，经历春节假期，2019 年 3、4 月份第一代系统整体上线；2020 年初着手升级方案调研，经历春节及疫情，2020 年 4 月份开始第二代系统的整体升级。
本文将会简述两代搜图系统背后的技术选型及基本原理。
基础概要 Link to heading 图像是什么？ Link to heading 与图像打交道，我们必须要先知道：图像是什么？
答案：像素点的集合。
比如：
左图红色圈中的部分其实就是右图中一系列的像素点。
再举例：
假设上图红色圈的部分是一幅图像，其中每一个独立的小方格就是一个像素点（简称像素），像素是最基本的信息单元，而这幅图像的大小就是 11 x 11 px 。
图像的数学表示 Link to heading 每个图像都可以很自然的用矩阵来表示，每个像素点对应的就是矩阵中的一个元素。
二值图像 Link to heading 二值图像的像素点只有黑白两种情况，因此每个像素点可以由 0 和 1 来表示。
比如一张 4 * 4 二值图像：
0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 RGB 图像 Link to heading 红（Red）、绿（Green）、蓝（Blue）作为三原色可以调和成任意的颜色，对于 RGB 图像，每个像素点包含 RGB 共三个通道的基本信息，类似的，如果每个通道用 8 bit 表示即 256 级灰度，那么一个像素点可以表示为：</description></item><item><title>以图搜图系统工程实践</title><link>https://rifewang.github.io/posts/engineering/image-search-system2/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/engineering/image-search-system2/</guid><description>以图搜图系统工程实践 Link to heading 之前写过一篇概述: 以图搜图系统概述 。
以图搜图系统需要解决的主要问题是：
提取图像特征向量（用特征向量去表示一幅图像） 特征向量的相似度计算（寻找内容相似的图像） 对应的工程实践，具体为：
卷积神经网络 CNN 提取图像特征 向量搜索引擎 Milvus CNN Link to heading 使用卷积神经网路 CNN 去提取图像特征是一种主流的方案，具体的模型则可以使用 VGG16 ，技术实现上则使用 Keras + TensorFlow ，参考 Keras 官方示例：
from keras.applications.vgg16 import VGG16 from keras.preprocessing import image from keras.applications.vgg16 import preprocess_input import numpy as np model = VGG16(weights=&amp;#39;imagenet&amp;#39;, include_top=False) img_path = &amp;#39;elephant.jpg&amp;#39; img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.</description></item><item><title>以图搜图系统概述</title><link>https://rifewang.github.io/posts/engineering/image-search-system/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/engineering/image-search-system/</guid><description>以图搜图系统概述 Link to heading 以图搜图指的是根据图像内容搜索出相似内容的图像。
构建一个以图搜图系统需要解决两个最关键的问题：首先，提取图像特征；其次，特征数据搜索引擎，即特征数据构建成数据库并提供相似性搜索的功能。
图像特征表示 Link to heading 介绍三种方式。
图像哈希 Link to heading 图像通过一系列的变换和处理最终得到的一组哈希值称之为图像的哈希值，而中间的变换和处理过程则称之为哈希算法。
图像的哈希值是对这张图像的整体抽象表示。
比如 Average Hash 算法的计算过程： Reduce size : 将原图压缩到 8 x 8 即 64 像素大小，忽略细节。 Reduce color : 灰度处理得到 64 级灰度图像。 Average the colors : 计算 64 级灰度均值。 Compute the bits : 二值化处理，将每个像素与上一步均值比较并分别记为 0 或者 1 。 Construct the hash : 根据上一步结果矩阵构成一个 64 bit 整数，比如按照从左到右、从上到下的顺序。最后得到的就是图像的均值哈希值。 参考：http://www.hackerfactor.com/blog/?/archives/432-Looks-Like-It.html
图像哈希算法有很多种，包含但不限于:
AverageHash : 也叫 Different Hash PHash : Perceptual Hash MarrHildrethHash : Marr-Hildreth Operator Based Hash RadialVarianceHash : Image hash based on Radon transform BlockMeanHash : Image hash based on block mean ColorMomentHash : Image hash based on color moments 我们最常见可能就是 PHash 。</description></item><item><title>GitHub Actions 指南</title><link>https://rifewang.github.io/posts/uncate/github-actions/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/github-actions/</guid><description>GitHub Actions 指南 Link to heading GitHub Actions 使你可以直接在你的 GitHub 库中创建自定义的工作流，工作流指的就是自动化的流程，比如构建、测试、打包、发布、部署等等，也就是说你可以直接进行 CI（持续集成）和 CD （持续部署）。
基本概念 Link to heading workflow : 一个 workflow 工作流就是一个完整的过程，每个 workflow 包含一组 jobs 任务。 job : jobs 任务包含一个或多个 job ，每个 job 包含一系列的 steps 步骤。 step : 每个 step 步骤可以执行指令或者使用一个 action 动作。 action : 每个 action 动作就是一个通用的基本单元。 配置 workflow Link to heading workflow 必须存储在你的项目库根路径下的 .github/workflows 目录中，每一个 workflow 对应一个具体的 .yml 文件（或者 .yaml）。
workflow 示例：
name: Greet Everyone # This workflow is triggered on pushes to the repository.</description></item><item><title>给你的库加上酷炫的小徽章</title><link>https://rifewang.github.io/posts/uncate/ava-codecov-travis/</link><pubDate>Sat, 21 Dec 2019 13:36:00 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/ava-codecov-travis/</guid><description>给库加上酷炫的小徽章 &amp;amp; ava、codecov、travis 示例 Link to heading GitHub 很多开源库都会有几个酷炫的小徽章，比如：
这些是怎么加上去的呢？
Shields.io Link to heading 首先这些徽章可以直接去 shields.io 网站自动生成。
比如：
就是 version 这一类里的一种图标，选择 npm 一栏填入包名，然后复制成 Markdown 内容，就会得到诸如：
![npm (tag)](https://img.shields.io/npm/v/io-memcached/latest) 直接粘贴在 .md 文件中就可以使用了，最后展现的就是这个图标。
当然还有其他很多徽章都任由你挑选，不过某些徽章是需要额外进行一些配置，比如这里的 (自动构建通过) 和 (测试覆盖率)。
AVA Link to heading 谈到测试覆盖率必须先有单元测试，本文使用 ava 作为示例，ava 是一个 js 测试库，强烈推荐你使用它。
1、安装
npm init ava 2、使用示例
编写 test.js 文件：
import test from &amp;#39;ava&amp;#39; import Memcached from &amp;#39;../lib/memcached&amp;#39;; test.before(t =&amp;gt; { const memcached = new Memcached([&amp;#39;127.0.0.1:11211&amp;#39;], { pool: { max: 2, min: 0 }, timeout: 5000 }); t.</description></item><item><title>使用 Makefile 构建指令集</title><link>https://rifewang.github.io/posts/uncate/makefile/</link><pubDate>Sun, 15 Dec 2019 13:39:47 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/makefile/</guid><description>使用 Makefile 构建指令集 Link to heading make 是一个历史悠久的构建工具，通过配置 Makefile 文件就可以很方便的使用你自己自定义的各种指令集，且与具体的编程语言无关。 例如配置如下的 Makefile :
run dev: NODE_ENV=development nodemon server.js 这样当你在命令行执行 make run dev 时其实就会执行 NODE_ENV=development nodemon server.js 指令。
使用 Makefile 构建指令集可以很大的提升工作效率。
Makefile 基本语法 Link to heading &amp;lt;target&amp;gt;: &amp;lt;prerequisites&amp;gt; &amp;lt;commands&amp;gt; target 其实就是执行的目标，prerequisites 是执行这条指令的前置条件，commands 就是具体的指令内容。
示例：
build: clean go build -o myapp main.go clean: rm -rf myapp 这里的 build 有一个前置条件 clean ，意思就是当你执行 make build 时，会先执行 clean 的指令内容 rm -rf myapp ，然后再执行 build 的内容 go build -o myapp main.</description></item><item><title>实现 memcached 客户端：TCP、连接池、一致性哈希、自定义协议</title><link>https://rifewang.github.io/posts/uncate/create-memcached-client/</link><pubDate>Mon, 09 Dec 2019 13:41:38 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/create-memcached-client/</guid><description>实现 memcached 客户端：TCP、连接池、一致性哈希、自定义协议。 Link to heading 废话不多说，文本将带你实现一个简单的 memcached 客户端。
集群：一致性哈希 Link to heading memcached 本身并不支持集群，为了使用集群，我们可以自己在客户端实现路由分发，将相同的 key 路由到同一台 memcached 上去即可。 路由算法有很多，这里我们使用一致性哈希算法。
一致性哈希算法的原理：
一致性哈希算法已经有开源库 hashring 实现，基本用法：
const HashRing = require(&amp;#39;hashring&amp;#39;); // 输入集群地址构造 hash ring const ring = new HashRing([&amp;#39;127.0.0.1:11211&amp;#39;, &amp;#39;127.0.0.2:11211&amp;#39;]); // 输入 key 获取指定节点 const host = ring.get(key); TCP 编程 Link to heading 包括 memcached 在内的许多系统对外都是通过 TCP 通信。在 Node.js 中建立一个 TCP 连接并进行数据的收发很简单：
const net = require(&amp;#39;net&amp;#39;); const socket = new net.Socket(); socket.</description></item><item><title>时序数据库 InfluxDB（七）</title><link>https://rifewang.github.io/posts/influxdb/7/</link><pubDate>Sun, 17 Nov 2019 13:43:48 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/7/</guid><description> 单点故障和容灾备份 Link to heading InfluxDB 开源的社区版本面临的最大的问题就是单点故障和容灾备份，有没有一个简单的方案去解决这个问题呢？
既然有单点故障的可能，那么索性写入多个节点，同时也解决了容灾备份的问题：
1、在不同的机器上配置多个 InfluxDB 实例，写入数据时，直接由客户端并发写入多个实例。（为什么不用代理，因为代理自身就是个单点）。
2、当某个 InfluxDB 实例故障而导致写入失败时，记录失败的数据和节点，这些失败的数据可以临时存储在数据库、消息中间件、日志文件等等里面。
3、通过自定义的 worker 拉取上一步记录的失败的数据然后重写这些数据。
4、多个 InfluxDB 中的数据最终一致。
当然你需要注意的是：
1、由于是并发写入多个节点，且不同机器的状况不一，所以写入数据应该设置一个超时时间。
2、写入失败的数据必须要与节点相对应，同时你应该考虑如何去定义失败的数据：由于格式不正确或者权限问题导致的 4xx 或者 InfluxDB 本身异常导致的 5xx ，这些与 InfluxDB 宕机等故障导致的失败显然是不同的。
3、由于失败的数据需要临时存储在一个数据容器中，你应该考虑所使用的数据容器能否承载故障期间写入的数据压力，以及如果数据要求不可丢失，那么数据容器也需要有对应的支持。
4、失败数据的重写是一个异步的过程，所以写入的数据应该由客户端指定明确的时间戳，而不是使用 InfluxDB 写入时默认生成的时间戳。
5、故障期间多个 InfluxDB 可能存在数据不一致的情况。
相关文章：
时序数据库 InfluxDB（一） 时序数据库 InfluxDB（二） 时序数据库 InfluxDB（三） 时序数据库 InfluxDB（四） 时序数据库 InfluxDB（五） 时序数据库 InfluxDB（六） 时序数据库 InfluxDB（七）</description></item><item><title>时序数据库 InfluxDB（六）</title><link>https://rifewang.github.io/posts/influxdb/6/</link><pubDate>Wed, 06 Nov 2019 13:36:39 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/6/</guid><description>CQ 连续查询 Link to heading 连续查询 Continuous Queries（ CQ ）是 InfluxDB 很重要的一项功能，它的作用是在 InfluxDB 数据库内部自动定期的执行查询，然后将查询结果存储到指定的 measurement 里。
配置文件中的相关配置：
[continuous_queries] enabled = true log-enabled = true query-stats-enabled = false run-interval = &amp;#34;1s&amp;#34; enabled = true ：开启CQ log-enabled = true ：输出 CQ 日志 query-stats-enabled = false ：关闭 CQ 执行相关的监控，不会将统计数据写入默认的监控数据库 _internal run-interval = &amp;ldquo;1s&amp;rdquo; ：InfluxDB 每隔 1s 检查是否有 CQ 需要执行 基本语法 Link to heading 一 、 Link to heading 基本语法：
CREATE CONTINUOUS QUERY &amp;lt;cq_name&amp;gt; ON &amp;lt;database_name&amp;gt; BEGIN &amp;lt;cq_query&amp;gt; END 在某个数据库上创建一个 CQ ，而查询的具体内容 cq_query 的语法为：</description></item><item><title>时序数据库 InfluxDB（五）</title><link>https://rifewang.github.io/posts/influxdb/5/</link><pubDate>Wed, 30 Oct 2019 13:33:30 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/5/</guid><description>系统监控 Link to heading InfluxDB 自带有一个监控系统，默认情况下此功能是开启的，每隔 10 秒中采集一次系统数据并把数据写入到 _internal 数据库中，其默认使用名称为 monitor 的 RP（数据保留 7 天），相关配置见配置文件中的：
[monitor] store-enabled = true store-database = &amp;#34;_internal&amp;#34; store-interval = &amp;#34;10s&amp;#34; _internal 数据库与其它数据库的使用方式完全一致，其记录的统计数据分为多个 measurements ：
cq ：连续查询 database ：数据库 httpd ：HTTP 相关 queryExecutor ：查询执行器 runtime ：运行时 shard ：分片 subscriber ：订阅者 tsm1_cache ：TSM cache 缓存 tsm1_engine ：TSM 引擎 tsm1_filestore ：TSM filestore tsm1_wal ：TSM 预写日志 write ：数据写入 比如查询最近一次统计的数据写入情况：
select * from &amp;#34;write&amp;#34; order by time desc limit 1 _internal 数据库里的这些 measurements 中具体有哪些 field ，每个 field 数据又代表了什么含义，请参考官方文档：</description></item><item><title>时序数据库 InfluxDB（四）</title><link>https://rifewang.github.io/posts/influxdb/4/</link><pubDate>Mon, 28 Oct 2019 13:27:36 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/4/</guid><description>存储引擎 Link to heading InfluxDB 数据的写入如下图所示：
所有数据先写入到 WAL（ Write Ahead Log ）预写日志文件，并同步到 Cache 缓存中，当 Cache 缓存的数据达到了一定的大小，或者达到一定的时间间隔之后，数据会被写入到 TSM 文件中。
为了更高效的存储大量数据，存储引擎会将数据进行压缩处理，压缩的输入和输出都是 TSM 文件，因此为了以原子方式替换以及删除 TSM 文件，存储引擎由 FileStore 负责调节对所有 TSM 文件的访问权限。
Compaction Planner 负责确定哪些 TSM 文件已经准备好了可以进行压缩，并确保多个并发压缩不会互相干扰。
Compactor 压缩器则负责具体的 Compression 压缩工作。
为了处理文件，存储引擎通过 Writers/Readers 处理数据的写和读。另外存储引擎还会使用 In-Memory Index 内存索引快速访问 measurements、tags、series 等数据。
存储引擎的组成部分：
In-Memory Index ：跨分片的共享内存索引，并不是存储引擎本身特有的，存储引擎只是用到了它。 WAL ：预写日志。 Cache ：同步缓存 WAL 的内容，并最终刷写到 TSM 文件中去。 TSM Files ：特定格式存储最终数据的磁盘文件。 FileStore ：调节对磁盘上所有TSM文件的访问。 Compactor ：压缩器。 Compaction Planner ：压缩计划。 Compression ：编码解码压缩。 Writers/Readers ：读写文件。 硬件指南 Link to heading 为了应对不同的负载情况，我需要机器具有怎样的硬件配置？</description></item><item><title>时序数据库 InfluxDB（三）</title><link>https://rifewang.github.io/posts/influxdb/3/</link><pubDate>Sun, 27 Oct 2019 13:25:48 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/3/</guid><description>数据类型 Link to heading InfluxDB 是一个无结构模式，这也就是说你无需事先定义好表以及表的数据结构。
InfluxDB 支持的数据类型非常简单：
measurement : string tag key : string tag value : string field key : string field value : string , float , interger , boolean 你可以看到除了 field value 支持的数据类型多一点之外，其余全是字符串类型。
当然还有最重要的 timestamp ，InfluxDB 中的时间都是 UTC 时间，而且时间精度非常高，默认为纳秒。
数据结构设计 Link to heading 在实际使用中，数据都是存储在 tag 或者 field 中，这两者最重要的区别就是，tag 会构建索引（也就是说查询时，where 条件里的是 tag ，则查询性能更高），field 则不会被索引。
存储数据到底是使用 tag 还是 field ，参考以下原则：
常用于查询条件的数据存储为 tag 。 计划使用 GROUP BY() 的数据存储为 tag 。 计划使用 InfluxQL function 的数据存储为 field 。 数据不只是 string 类型的存储为 field 。 对于标识性的名称，如 database、RP、user、measurement、tag key、field key 这些应该避免使用 InfluxQL 中的关键字。</description></item><item><title>时序数据库 InfluxDB（二）</title><link>https://rifewang.github.io/posts/influxdb/2/</link><pubDate>Sat, 26 Oct 2019 13:14:35 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/2/</guid><description>RP Link to heading 先回顾一下 RP 策略（ retention policy ），它由三个部分构成：
DURATION：数据的保留时长。 REPLICATION：集群模式下数据的副本数，单节点无效。 SHARD DURATION：可选项，shard group 划分的时间范围。 前两个部分没啥好说的，而 shard duration 和 shard group 的概念你可能会感到比较陌生。
shard 是什么？
先来看数据的层次结构：
如果所示，一个 database 对应一个实际的磁盘上的文件夹，该数据库下不同的 RP 策略对应不同的文件夹。
shard group 只是一个逻辑概念，并没有实际的磁盘文件夹，shard group 包含有一个或多个 shard 。
最终的数据是存储在 shard 中的，每个 shard 也对应一个具体的磁盘文件目录，数据是按照时间范围分割存储的，shard duration 也就是划分 shard group 的时间范围（例如 shard duration 如果是一周，那么第一周的数据就会存储到一个 shard group 中，第二周的数据会存储到另外一个 shard group 中，以此类推）。
另外，每个 shard 目录下都有一个 TSM 文件（后缀名为 .tsm ），正是这个文件存储了最后编码和压缩后的数据。shard group 下的 shard 是按照 series 来划分的，每个 shard 包含一组特定的 series ，换句话说特定 shard group 中的特定 series 上的所有 points 点都存储在同一个 TSM 文件中。</description></item><item><title>时序数据库 InfluxDB（一）</title><link>https://rifewang.github.io/posts/influxdb/1/</link><pubDate>Fri, 25 Oct 2019 13:02:58 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/1/</guid><description>数据库种类有很多，比如传统的关系型数据库 RDBMS（ 如 MySQL ），NoSQL 数据库（ 如 MongoDB ），Key-Value 类型（ 如 redis ），Wide column 类型（ 如 HBase ）等等等等，当然还有本系列文章将会介绍的时序数据库 TSDB（ 如 InfluxDB ）。
时序数据库 TSDB Link to heading 不同的数据库针对的应用场景有不同的偏重。TSDB（ time series database ）时序数据库是专门以时间维度进行设计和优化的。 TSDB 通常具有以下的特点：
时间是不可或缺的绝对主角（就像 MySQL 中的主键一样），数据按照时间顺序组织管理 高并发高吞吐量的数据写入 数据的更新很少发生 过期的数据可以批量删除 InfluxDB 就是一款非常优秀的时序数据库，高居 DB-Engines TSDB rank 榜首。
InfluxDB 分为免费的社区开源版本，以及需要收费的闭源商业版本，目前只有商业版本支持集群。
InfluxDB 的底层数据结构从 LSM 树到 B+ 树折腾了一通，最后自创了一个 TSM 树（ Time-Structured Merge Tree ），这也是它性能高且资源占用少的重要原因。
InfluxDB 由 go 语言编写而成，没有额外的依赖，它的查询语言 InfluxQL 与 SQL 极其相似，使用特别简单。
InfluxDB 基本概念 Link to heading InfluxDB 有以下几个核心概念： 1、database ： 数据库。</description></item><item><title>Dockerfile 最佳实践</title><link>https://rifewang.github.io/posts/kubernetes/dockerfile-best-practice/</link><pubDate>Wed, 10 Jul 2019 16:32:36 +0800</pubDate><guid>https://rifewang.github.io/posts/kubernetes/dockerfile-best-practice/</guid><description>Dockerfile 是用来构建 docker 镜像的配置文件，语法简单上手容易，你可以很轻松的就编写一个能正常使用的 Dockerfile ，但是它很有可能还不够好，本文将会从细节上介绍一些 tips 助你实现最佳实践。
1、注意构建顺序
FROM debian - COPY ../app RUN apt-get update RUN apt-get -y install cron vim ssh + COPY ../app 上例中第二步一旦本地文件发生了变化将会导致包括此后步骤的缓存全部失效，必须重新构建，尤其是在开发环境，这将会增加你构建镜像的耗时。构建步骤的排序很重要，改变小的步骤放前面，改变大的步骤放后面，这有助于你优化使用缓存加速整个构建过程。
2、使用更精确的 COPY
只 copy 真正需要的文件，比如 node_modules 或者其它一些对于构建镜像毫无作用的文件一定要忽略掉（写入 .dockerignore 文件），这些无用的文件百害而无一利。
3、合并指令
- RUN apt-get update - RUN apt-get -y install cron vim ssh + RUN apt-get update \ &amp;amp;&amp;amp; apt-get -y install cron vim ssh 像这种 apt-get 升级和安装分为两个步骤毫无必要，反之统一为一个步骤更有利于缓存。你如果仔细观察各种官方镜像的 Dockerfile 是怎么写的，你肯定会发现他们单条 RUN 指令的内容相当的冗长也不会拆分，这样写是有道理的。
4、移除不必要的依赖</description></item><item><title>Let's Encrypt 配置 HTTPS 免费泛域名证书</title><link>https://rifewang.github.io/posts/uncate/lets-encrypt/</link><pubDate>Thu, 27 Jun 2019 16:26:45 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/lets-encrypt/</guid><description>想要使用 HTTPS ，你必须先拥有权威 CA（证书签发机构）签发的证书（对于自签名的证书，浏览器是不认账的）。Let&amp;rsquo;s Encrypt 就是一家权威的 CA 证书签发机构，你可以向他申请免费的证书（一般商业证书的价格比较贵）。
推荐使用 acme.sh 这个工具，申请泛域名证书示例：
注意：以下示例中，我的二级域名是 rifewang.club （一般你向云服务商购买的都是二级域名），泛域名是 *.x.rifewang.club 。
1、在系统上安装 acme.sh ，默认安装位置是 ~/.acme.sh :
curl https://get.acme.sh | sh 安装要求系统必须已经安装了 cron , crontab , crontabs , vivie-cron 其中任意一个工具，不然会提示你安装失败，没有的话先安装一个即可。
注意：以下操作使用的是 DNS manual mode 的方式。
2、发起 issue 申请获取域名 DNS TXT 记录：
acme.sh --issue --force --dns -d &amp;lt;二级域名&amp;gt; -d &amp;lt;泛域名&amp;gt; \ --yes-I-know-dns-manual-mode-enough-go-ahead-please 注意：你必须先将 acme.sh 这个可执行文件的路径添加到系统的环境变量 PATH 中，或者直接在可执行文件目录下执行，否则肯定会提示你 acme.sh command not found 。
&amp;ndash;force 强制 issue ，某些情况下你的域名已经验证成功了就会跳过验证，不会生成新的 TXT 记录，所以这里强制执行一下。</description></item><item><title>深入理解 Node.js 事件循环架构</title><link>https://rifewang.github.io/posts/uncate/nodejs-event-loop-architecture/</link><pubDate>Tue, 28 May 2019 15:53:48 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/nodejs-event-loop-architecture/</guid><description>关于 Node.js ，相信你已经了解过不少内容，诸如 Node.js 内核、事件循环、单线程、setTimeout 或 setImmediate 函数的执行机制等等。
当然最重要的，你应该知道 Node.js 使用的是非阻塞 IO 模型以及异步的编程风格。本文仍将深入核心进行相关内容的探讨。
01 Link to heading 事件循环到底是什么？Node.js 到底是单线程还是多线程？
关于这个问题，网络上充斥着各种不清晰甚至错误的答案。本文将会深入 Node.js 内核，阐述它是如何实现的以及它的工作机制。 Node.js 并不仅仅只是 &amp;quot; JavaScript on the Server &amp;quot; ，更重要的是，其中约 30% 的部分是 C++ 而不是 JS 。本文将会讲述这些 C++ 部分在 Node.js 中实际做了什么。
Node.js 是单线程？
答案：Node.js 既是单线程，但同时也不是。
一些相关名词：multitasking（多任务）、single-threaded（单线程）、multi-threaded（多线程），thread pool（线程池）、epoll loop（epoll 循环）、event loop（事件循环）。
让我们从头开始深入了解 Node.js 内核中发生了什么？
处理器可以一次处理一件事，也可以一次并行地处理多个任务（multitasking）。 对于单核处理器，其只能一次处理一个任务，应用程序在完成任务后调用 yield 去通知处理器开始处理下一个任务，就像 JavaScript 中的 generator 函数一样，否则没有 yield 则将返回当前任务。在过去，当应用程序无法调用 yield 时，其服务将处于无法访问的状态。
进程是一个 top level 执行容器，它有自己专用的内存系统。 这意味着在一个进程中无法直接获取另一个进程的内存中的数据，为了使两个进程进行通信，我们必须要另外做一些工作，称之为 inter-process communication（ IPC ，进程间通信），它依赖于 system sockets（系统套接字）。</description></item><item><title>流平台 Kafka</title><link>https://rifewang.github.io/posts/middleware/kafka/</link><pubDate>Thu, 11 Apr 2019 14:49:58 +0800</pubDate><guid>https://rifewang.github.io/posts/middleware/kafka/</guid><description>简介 Link to heading Kafka 作为一个分布式的流平台，正在大数据相关领域得到越来越广泛的应用，本文将会介绍 kafka 的相关内容。
流平台如 kafka 具备三大关键能力：
发布和订阅消息流，类似于消息队列。 以容错的方式存储消息流。 实时处理消息流。 kafka 通常应用于两大类应用：
构建实时数据流管道，以可靠的获取系统或应用之间的数据。 构建实时转换或响应数据流的应用程序。 kafka 作为一个消息系统，可以接受 producer 生产者投递消息，以及 consumer 消费者消费消息。
kafka 作为一个存储系统，会将所有消息以追加的方式顺序写入磁盘，这意味着消息是会被持久化的，传统消息队列中的消息一旦被消费通常都会被立即删除，而 kafka 却并不会这样做，kafka 中的消息是具有存活时间的，只有超出存活时间才会被删除，这意味着在 kafka 中能够进行消息回溯，从而实现历史消息的重新消费。
kafka 的流处理，可以持续获取输入流的数据，然后进行加工处理，最后写入到输出流。kafka 的流处理强依赖于 kafka 本身，并且只是一个类库，与当前知名的流处理框架如 spark 和 flink 还是有不小的区别和差距。
大多数使用者以及本文重点关注的也只是 kafka 的前两种能力，下面将会对此进行更加详细的介绍。
相关概念 Link to heading kafka 中的相关概念如下图所示：
1、Producer ：生产者，投递消息。
2、Topic ：消息的逻辑分类，所有消息都必须归属于一个特定的 topic 主题。
3、Broker ：kafka 集群具有多个 broker（代理节点），一个 broker 其实就是一个 kafka 服务器。
4、Partition ：topic 只是逻辑上的概念，每个 topic 主题下的消息都会被分开存储在多个 partition 分区中，为了容错，kafka 提供了备份机制，每个 partition 可以设置多个 replication 副本。</description></item><item><title>使用 Puppeteer 构建自动化端到端测试</title><link>https://rifewang.github.io/posts/uncate/puppeteer/</link><pubDate>Fri, 22 Mar 2019 15:04:16 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/puppeteer/</guid><description>端到端测试指的是将系统作为一个黑盒，模拟正常用户行为，跨越从前端到后端整个软件系统，是一种全局性的整体测试。
来看本文的示例：
There should have been a video here but your browser does not seem to support it. 你在视频中看到的所有操作全部都是由程序自动完成的，就像真实的用户一样，通过这种自动化的方式可以很好的提升我们的测试效率从而保证交付的质量。
完成这样的操作相当简单，只需要 Puppeteer 就够了。Puppeteer 是一个 node 库，通过它提供的高级 API 便可以控制 chromium 或者 chrome ，换句话说，在浏览器中进行的绝大部分人工操作都可以通过在 node 程序中调用 Puppeteer 的 API 来完成。
本文示例中的所有操作无外乎于：
获取页面元素 键盘输入 鼠标操作 文件上传 执行原生JS 一、打开浏览器跳转页面：
const browser = await puppeteer.launch({ headless: false, // 打开浏览器 defaultViewport: { // 设置视窗宽高 width: 1200, height: 800 } }); const page = await browser.newPage(); await page.goto(url); // 跳转页面 二、获取输入框并输入：</description></item><item><title>图像相似性：哈希和特征</title><link>https://rifewang.github.io/posts/uncate/image-similarity/</link><pubDate>Thu, 14 Mar 2019 15:15:10 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/image-similarity/</guid><description>引言 Link to heading 如何判断图像的相似性？
直接比较图像内容的 md5 值肯定是不行的，md5 的方式只能判断像素级别完全一致。图像的基本单元是像素，如果两张图像完全相同，那么图像内容的 md5 值一定相同，然而一旦小部分像素发生变化，比如经过缩放、水印、噪声等处理，那么它们的 md5 值就会天差地别。
本文将会介绍图像相似性的两大有关概念：图像哈希、图像特征。
图像哈希 Link to heading 图像通过一系列的变换和处理最终得到的一组哈希值称之为图像的哈希值，而中间的变换和处理过程则称之为哈希算法。
下面以 Average Hash 算法为例描述这一基本过程：
1、Reduce size : 将原图压缩到 8 x 8 即 64 像素大小，忽略细节。
2、Reduce color : 灰度处理得到 64 级灰度图像。
3、Average the colors : 计算 64 级灰度均值。
4、Compute the bits : 二值化处理，将每个像素与上一步均值比较并分别记为 0 或者 1 。
5、Construct the hash : 根据上一步结果矩阵构成一个 64 bit 整数，比如按照从左到右、从上到下的顺序。最后得到的就是图像的均值哈希值。
参考：http://www.hackerfactor.com/blog/?/archives/432-Looks-Like-It.html
如果你稍加留意，就会发现 Average Hash 均值哈希算法的处理过程相当简单，优点就是计算速度快，缺点就是局限性比较明显。
当然计算机视觉领域发展到现在已经有了多种图像哈希算法，OpenCV 支持的图像哈希算法包括：</description></item><item><title>图像处理基础</title><link>https://rifewang.github.io/posts/uncate/image-processing/</link><pubDate>Tue, 26 Feb 2019 14:51:44 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/image-processing/</guid><description>图像处理基础 Link to heading 现如今我们每时每刻都在与图像打交道，而图像处理也是我们绕不开的问题，本文将会简述图像处理的基础知识以及对常见的裁剪、画布、水印、平移、旋转、缩放等处理的实现。
在进行图像处理之前，我们必须要先回答这样一个问题：什么是图像？
答案是像素点的集合。
如上图所示，假设红色圈的部分是一幅图像，其中每一个独立的小方格就是一个像素点（简称像素），像素是最基本的信息单元，而这幅图像的大小就是 11 x 11 px 。
1、二值图像：
图像中的每个像素点只有黑白两种状态，因此每个像素点的信息可以用 0 和 1 来表示。
2、灰度图像：
图像中的每个像素点在黑色和白色之间还有许多级的颜色深度（表现为灰色），通常我们使用 8 个 bit 来表示灰度级别，因此总共有 2 ^ 8 = 256 级灰度，所以可以使用 0 到 255 范围内的数字来对应表示灰度级别。
3、RGB图像：
红（Red）、绿（Green）、蓝（Blue）作为三原色可以调和成任意的颜色，对于 RGB 图像，每个像素点包含 RGB 共三个通道的基本信息，类似的，如果每个通道用 8 bit 表示即 256 级灰度，那么一个像素点可以表示为：
([0 ... 255], [0 ... 255], [0 ... 255]) 图像矩阵：
每个图像都可以很自然的用矩阵来表示，每个像素对应矩阵中的每个元素。
例如：
1、4 x 4 二值图像：
0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 2、4 x 4 灰度图像：</description></item><item><title>Elasticsearch 入门指南</title><link>https://rifewang.github.io/posts/elasticsearch/es-guide/</link><pubDate>Sun, 29 Jul 2018 16:24:53 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-guide/</guid><description>引言 Link to heading Elasticsearch 是什么？一个开源的可扩展、高可用、分布式的全文搜索引擎。
你为什么需要它？《人生一串》中有这样一段话：
没了烟火气，人生就是一段孤独的旅程。
而我们如何通过烟火气、人生或者旅程等这样的关键词来搜索出这部纪录片呢？显然无论是传统的关系型数据库，还是 NOSQL 数据库都无法实现这样的需求，而这里 Elasticsearch 就派上了用场。
再来理解全文搜索是什么？举例来说，就是将上面那段话按照语义拆分成不同的词组并记录其出现的频率（专业术语叫构建倒排索引），这样当你输入一个简单的关键词就能将其搜索出来。
总而言之，Elasticsearch 就是为搜索而生。
一、基本概念 Link to heading Near Realtime（近实时） Elasticsearch 是一个近实时的搜索平台。为什么是近实时？在传统的数据库中一旦我们插入了某条数据，则立刻可以搜索到它，这就是实时。反之在 Elasticsearch 中为某条数据构建了索引（插入数据的意思）之后，并不能立刻就搜索到，因为它在底层需要进行构建倒排索引、将数据同步到副本等等一系列操作，所以是近实时（通常一秒以内，无需过于担心）。
Cluster（集群）&amp;amp; Node（节点） 每一个单一的 Elasticsearch 服务器称之为一个 Node 节点，而一个或多个 Node 节点则组成了 Cluster 集群。Cluster 和 Node 一定是同时存在的，换句话说我们至少拥有一个由单一节点构成的集群，而在实际对外提供索引和搜索服务时，我们应该将 Cluster 集群视为一个基本单元。
Cluster 集群默认的名称就是 elasticsearch ，而 Node 节点默认的名称是一个随机的 UUID ，我们只要将不同 Node 节点的 cluster name 设置为同一个名称便构成了一个集群（不论这些节点是否在同一台服务器上，只要网络有效可达，Elasticsearch 本身会自己去搜索并发现这些节点并构成集群）。
Index（索引）&amp;amp; Type（类型）&amp;amp; Document（文档） Document（文档）是最基本的数据单元，我们可以将其理解为 mysql 中的具体的某一行数据。
Type（类型）在 6.0 版本之后被移除，它是一个逻辑分类，我们可以将其理解为 mysql 中的某一张表。
Index（索引）是具有类似特征的 Document 文档的集合，我们可以将其理解为 mysql 中的某一个数据库。</description></item><item><title>消息队列 NSQ 入门指南</title><link>https://rifewang.github.io/posts/middleware/nsq/</link><pubDate>Sun, 08 Jul 2018 11:17:45 +0800</pubDate><guid>https://rifewang.github.io/posts/middleware/nsq/</guid><description>一 Link to heading NSQ 是什么？使用 go 语言开发的一款开源的消息队列，具有轻量级、高性能的特点。
概述 Link to heading NSQ 组件：
1、nsqd：接受、排队、传递消息的守护进程，消息队列中的核心。
2、nsqlookupd：管理拓扑信息，其实就是围绕 nsqd 的发现服务，因为其存储了 nsqd 节点的注册信息，所以通过它就可以查询到指定 topic 主题的 nsqd 节点。
3、nsqadmin：一套封装好的 WEB UI ，可以看到各种统计数据并进行管理操作。
4、utilities：封装好的一些简单的工具（实际开发中用的不多）。
如下图所示：
1、生产者 producer 将消息投递到指定的 nsqd 中指定的 topic 主题。
2、nsqd 可以有多个 topic 主题，一旦其接受到消息，将会把消息广播到所有与这个 topic 相连的 channel 队列中。
3、channel 队列接收到消息则会以负载均衡的方式随机的将消息传递到与其连接的所有 consumer 消费者中的某一个。
注意：生产者关注的是 topic，消费者关注的是 channel。消息是存在 channel 队列中的，其会一直保存消息直到有消费者将消息消费掉，同时 channel 队列一旦创建其本身也不会自动消失，另外消息默认是存在内存中的，一旦超过内存大小（可通过 &amp;ndash;mem-queue-size 配置）则会被存储到磁盘上。
再看下图：
通过 nsqadmin 可以看到整个集群的统计信息并进行管理，多个 nsqd 节点组成集群并将其基本信息注册到 nsqlookupd 中，通过 nsqlookupd 可以寻址到具体的 nsqd 节点，而不论是消息的生产者还是消费者，其本质上都是与 nsqd 进行通信（如第一张图所示）。</description></item><item><title>Docker 入门教程</title><link>https://rifewang.github.io/posts/kubernetes/docker-guide/</link><pubDate>Tue, 17 Apr 2018 10:50:21 +0800</pubDate><guid>https://rifewang.github.io/posts/kubernetes/docker-guide/</guid><description>一 Link to heading 程序明明在我本地跑得好好的，怎么部署上去就出问题了？如果要在同一台物理机上同时部署多个 node 版本并独立运行互不影响，这又该怎么做？如何更快速的将服务部署到多个物理机上？
“Build once , run anywhere” ，既可以保证环境的一致性，同时又能更方便的将各个环境相互隔离，还能更快速的部署各种服务，这就是 docker 的能力。
基本概念 Link to heading 一张图慢慢讲：
1、本地开发写好了 code ，首先我们需要通过 build 命令构建 image 镜像，而构建的规则呢，就需要写在这个 dockerfile 文件里。
2、image 镜像是什么？静态的、只读的文件（先不着急，有个基本印象，后面再慢慢讲）。如何更方便的区分不同的镜像呢，通过 tag 命令给镜像打上标签就行了。
3、image 镜像存在哪里？通过 push 命令推送到 repository 镜像仓库，每个仓库可以存放多个镜像。
4、registry 是啥？仓库服务器，所有 repository 仓库都必须依赖于一个 registry 才能提供镜像存储的服务。我们在自己的物理机上安装一个 registry ，这样可以构建自己私有的镜像仓库了。
5、镜像光存到仓库里可没用，还要能部署并运行起来。
6、首先通过 pull 命令将仓库里的镜像拉到服务器上，然后通过 run 命令即可将这个镜像构建成一个 container 容器，容器又是什么？是镜像的运行时，读取镜像里的各种配置文件并如同一个小而独立的服务器一样运行你的各种服务。到这里，你的一个服务就算是部署并运行起来了。
7、数据怎么办？通过 volume 数据卷可以将容器使用的数据挂在到物理机本地，而各个容器之间相互传递处理数据呢，统一通过另一个 volume container 数据卷容器提供数据的服务，数据卷容器也只是一个普通的容器。
8、image 镜像怎么导入导出到本地？通过 save 命令即可导出成压缩包到物理机本地磁盘上，通过 load 命令就可以导入成 docker 环境下的镜像。</description></item><item><title>RabbitMQ 入门教程及示例</title><link>https://rifewang.github.io/posts/middleware/rabbitmq/</link><pubDate>Tue, 27 Feb 2018 18:53:07 +0800</pubDate><guid>https://rifewang.github.io/posts/middleware/rabbitmq/</guid><description>一 Link to heading 消息中间件 MQ（也称消息队列）的基本功能是传递和转发消息，其最重要的作用是能够解耦业务及系统架构，可以说是一个系统发展壮大到一定阶段绕不开的东西。
而 RabbitMQ 是对 AMQP（高级消息队列协议）的实现，成熟可靠并且开源，本系列文章将会讲述如何在 node 中入门这一利器。
RabbitMQ 概述 Link to heading 先来简单的了解一下 RabbitMQ 相关的基本概念：
Producer ：生产者，生成消息并把消息发送给 RabbitMQ 。
Consumer ：消费者，从 RabbitMQ 中接收消息。
Exchange ：交换器，具有路由的作用，将生产者传递的消息根据不同的路由规则传递到对应的队列中。交换器具有四种不同的类型，每种类型对应不同的路由规则。
Queue ：队列，实际存储消息的地方，消费者通过订阅队列来获取队列中的消息。
Binding ：绑定交换器和队列，只有绑定后消息才能被交换器分发到具体的队列中，用一个字符串来代表 Binding Key 。
消息是如何由生产者传递到消费者：
生产者 Producer 生成消息 msg ，并指定这条消息的路由键 Routing Key ，然后将消息传递给交换器 Exchange 。
交换器 Exchange 接收到消息后根据 Exchange Type 也就是交换器类型以及交换器和队列的 Binding 绑定关系来判断路由规则并分发消息到具体的队列 Queue 中。
消费者 Consumer 通过订阅具体的队列，一旦队列接收到消息便会将其传递给消费者。
这里的 Routing Key 和 Binding 我是按照自己的理解解释的，与某些参考资料是有出入的，读者理解就好。
当然完成上述三个步骤还缺少两个关键的东西：
Connection ：连接，不论生产者还是消费者想要使用 RabbitMQ 都必须首先建立到 RabbitMQ 的 TCP 连接。</description></item></channel></rss>