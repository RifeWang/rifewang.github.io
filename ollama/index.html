<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>AI LLM 利器 Ollama 架构和对话处理流程解析 - 凌虚 Blog</title><meta name=Description content="AI LLM 利器 Ollama 架构和对话处理流程解析"><meta property="og:url" content="https://rifewang.github.io/ollama/">
<meta property="og:site_name" content="凌虚 Blog"><meta property="og:title" content="AI LLM 利器 Ollama 架构和对话处理流程解析"><meta property="og:description" content="AI LLM 利器 Ollama 架构和对话处理流程解析"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-15T14:22:02+08:00"><meta property="article:modified_time" content="2024-10-15T19:43:22+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://rifewang.github.io/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rifewang.github.io/images/avatar.png"><meta name=twitter:title content="AI LLM 利器 Ollama 架构和对话处理流程解析"><meta name=twitter:description content="AI LLM 利器 Ollama 架构和对话处理流程解析"><meta name=application-name content="凌虚的博客"><meta name=apple-mobile-web-app-title content="凌虚的博客"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=canonical href=https://rifewang.github.io/ollama/><link rel=prev href=https://rifewang.github.io/k8s-pod-security/><link rel=next href=https://rifewang.github.io/first-run-of-ai-model/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"AI LLM 利器 Ollama 架构和对话处理流程解析","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/rifewang.github.io\/ollama\/"},"image":["https:\/\/rifewang.github.io\/images\/avatar.png"],"genre":"posts","keywords":"AI, LLM","wordcount":1220,"url":"https:\/\/rifewang.github.io\/ollama\/","datePublished":"2024-10-15T14:22:02+08:00","dateModified":"2024-10-15T19:43:22+08:00","license":"Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)","publisher":{"@type":"Organization","name":"凌虚","logo":"https:\/\/rifewang.github.io\/images\/avatar.png"},"author":{"@type":"Person","name":"凌虚"},"description":"AI LLM 利器 Ollama 架构和对话处理流程解析"}</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-VRMQFEVL7J"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VRMQFEVL7J")</script><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"light"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"light"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="凌虚 Blog">凌虚的博客</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/about>作者 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="凌虚 Blog">凌虚的博客</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/about title>作者</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">AI LLM 利器 Ollama 架构和对话处理流程解析</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>凌虚</a></span>&nbsp;<span class=post-category>included in <a href=/categories/ai/><i class="far fa-folder fa-fw" aria-hidden=true></i>AI</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-15>2024-10-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1220 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#ollama-概述>Ollama 概述</a><ul><li><a href=#ollama-整体架构>Ollama 整体架构</a></li><li><a href=#ollama-存储结构>Ollama 存储结构</a></li><li><a href=#ollama-对话处理流程>Ollama 对话处理流程</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></div><div class=content id=content><h2 id=ollama-概述>Ollama 概述</h2><p><code>Ollama</code> 是一个快速运行 <code>LLM</code>（Large Language Models，大语言模型）的简便工具。通过 <code>Ollama</code>，用户无需复杂的环境配置，即可轻松与大语言模型对话互动。</p><p>本文将解析 <code>Ollama</code> 的整体架构，并详细讲解用户在与 <code>Ollama</code> 进行对话时的具体处理流程。</p><h3 id=ollama-整体架构>Ollama 整体架构</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-cs.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-cs.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-cs.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-cs.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-cs.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-cs.png></p><p><code>Ollama</code> 使用了经典的 CS（Client-Server）架构，其中：</p><ul><li>Client 通过命令行的方式与用户交互。</li><li>Server 可以通过命令行、桌面应用（基于 Electron 框架）、Docker 其中一种方式启动。无论启动方式如何，最终都调用同一个可执行文件。</li><li>Client 与 Server 之间使用 HTTP 进行通信。</li></ul><p><code>Ollama Server</code> 有两个核心部分：</p><ul><li><code>ollama-http-server</code>：负责与客户端进行交互。</li><li><code>llama.cpp</code>：作为 LLM 推理引擎，负责加载并运行大语言模型，处理推理请求并返回结果。</li><li><code>ollama-http-server</code> 与 <code>llama.cpp</code> 之间也是通过 HTTP 进行交互。</li></ul><p>说明：<code>llama.cpp</code> 是一个独立的开源项目，具备跨平台和硬件友好性，可以在没有 GPU、甚至是树莓派等设备上运行。</p><h3 id=ollama-存储结构>Ollama 存储结构</h3><p><code>Ollama</code> 本地存储默认使用的文件夹路径为 <code>$HOME/.ollama</code>，文件结构如下图所示：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-dir.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-dir.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-dir.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-dir.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-dir.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-dir.png></p><p>文件可分为三类：</p><ul><li>日志文件：包括记录了用户对话输入的 <code>history</code> 文件，以及 <code>logs/server.log</code> 服务端日志文件。</li><li>密钥文件：id_ed25519 私钥和 id_ed25519.pub 公钥。</li><li>模型文件：包括 <code>blobs</code> 原始数据文件，以及 <code>manifests</code> 元数据文件。</li></ul><p>元数据文件，例如图中的 <code>models/manifests/registry.ollama.ai/library/llama3.2/latest</code> 文件内容为：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-manifest-file.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-manifest-file.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-manifest-file.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-manifest-file.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-manifest-file.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama-manifest-file.png></p><p>如上图所示，<code>manifests</code> 文件是 JSON 格式，文件内容借鉴了云原生和容器领域中的 OCI spec 规范，<code>manifests</code> 中的 digest 字段与 <code>blobs</code> 相对应。</p><h3 id=ollama-对话处理流程>Ollama 对话处理流程</h3><p>用户与 <code>Ollama</code> 进行对话的大致流程如下图所示：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama.drawio.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama.drawio.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama.drawio.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama.drawio.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama.drawio.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/ollama.drawio.png></p><ol><li>用户通过 CLI 命令行执行 <code>ollama run llama3.2</code> 开启对话（<code>llama3.2</code> 是一种开源的大语言模型，你也可以使用其它 LLM）。</li><li>准备阶段：<ul><li>CLI 客户端向 <code>ollama-http-server</code> 发起 HTTP 请求，获取模型信息，后者会尝试读取本地的 <code>manifests</code> 元数据文件，如果不存在，则响应 404 not found。</li><li>当模型不存在时，CLI 客户端会向 <code>ollama-http-server</code> 发起拉取模型的请求，后者会去远程存储仓库下载模型到本地。</li><li>CLI 再次请求获取模型信息。</li></ul></li><li>交互式对话阶段：<ul><li>CLI 先向 <code>ollama-http-server</code> 发起一个空消息的 <code>/api/generate</code> 请求，server 会先在内部进行一些 channel（go 语言中的通道）处理。</li><li>如果模型信息中包含有 messages，则打印出来。用户可以基于当前使用的模型和 session 对话记录保存为一个新的模型，而对话记录就会被保存为 messages。</li><li>正式进入对话：CLI 调用 <code>/api/chat</code> 接口请求 <code>ollama-http-server</code>，而 <code>ollama-http-server</code> 需要依赖 <code>llama.cpp</code> 引擎加载模型并执行推理（<code>llama.cpp</code> 也是以 HTTP server 的方式提供服务）。此时，<code>ollama-http-server</code> 会先向 <code>llama.cpp</code> 发起 <code>/health</code> 请求，确认后者的健康状况，然后再发起 <code>/completion</code> 请求，得到对话响应，并最终返回给 CLI 显示出来。</li></ul></li></ol><p>通过上述步骤，<code>Ollama</code> 完成了用户与大语言模型的交互对话。</p><h2 id=总结>总结</h2><p><code>Ollama</code> 通过集成 <code>llama.cpp</code> 推理引擎，并进一步封装，将复杂的 <code>LLM</code> 技术变得触手可及，为开发者和技术人员提供了一个高效且灵活的工具，很好地助力了各种应用场景下的大语言模型推理与交互。</p><p>(关注我，无广告，专注技术，不煽动情绪，也欢迎与我交流)</p><hr><p>参考资料：</p><ul><li><em><a href=https://github.com/ollama/ollama target=_blank rel="noopener noreffer">https://github.com/ollama/ollama</a></em></li><li><em><a href=https://github.com/ggerganov/llama.cpp target=_blank rel="noopener noreffer">https://github.com/ggerganov/llama.cpp</a></em></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2024-10-15</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://rifewang.github.io/ollama/ data-title="AI LLM 利器 Ollama 架构和对话处理流程解析" data-hashtags=AI,LLM><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://rifewang.github.io/ollama/ data-hashtag=AI><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://rifewang.github.io/ollama/ data-title="AI LLM 利器 Ollama 架构和对话处理流程解析"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://rifewang.github.io/ollama/ data-title="AI LLM 利器 Ollama 架构和对话处理流程解析"><i data-svg-src=/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://rifewang.github.io/ollama/ data-title="AI LLM 利器 Ollama 架构和对话处理流程解析"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/ai/>AI</a>,&nbsp;<a href=/tags/llm/>LLM</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/k8s-pod-security/ class=prev rel=prev title="Kubernetes：Seccomp、AppArmor、SELinux & Pod 安全性标准和准入"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Kubernetes：Seccomp、AppArmor、SELinux & Pod 安全性标准和准入</a>
<a href=/first-run-of-ai-model/ class=next rel=next title="只想简单跑个 AI 大模型，却发现并不简单">只想简单跑个 AI 大模型，却发现并不简单<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2017 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>凌虚</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/katex/katex.min.css><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:50,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>