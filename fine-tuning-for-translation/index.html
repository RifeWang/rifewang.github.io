<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调 - 凌虚 Blog</title><meta name=Description content="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><meta property="og:url" content="https://rifewang.github.io/fine-tuning-for-translation/">
<meta property="og:site_name" content="凌虚 Blog"><meta property="og:title" content="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><meta property="og:description" content="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-24T22:05:38+08:00"><meta property="article:modified_time" content="2024-10-25T00:08:22+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://rifewang.github.io/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rifewang.github.io/images/avatar.png"><meta name=twitter:title content="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><meta name=twitter:description content="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><meta name=application-name content="凌虚的博客"><meta name=apple-mobile-web-app-title content="凌虚的博客"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=canonical href=https://rifewang.github.io/fine-tuning-for-translation/><link rel=prev href=https://rifewang.github.io/k8s-gpu-deviceplugin-cdi-nfd/><link rel=next href=https://rifewang.github.io/build-my-chatgpt/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/rifewang.github.io\/fine-tuning-for-translation\/"},"image":["https:\/\/rifewang.github.io\/images\/avatar.png"],"genre":"posts","keywords":"AI, LLM","wordcount":1459,"url":"https:\/\/rifewang.github.io\/fine-tuning-for-translation\/","datePublished":"2024-10-24T22:05:38+08:00","dateModified":"2024-10-25T00:08:22+08:00","license":"Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)","publisher":{"@type":"Organization","name":"凌虚","logo":"https:\/\/rifewang.github.io\/images\/avatar.png"},"author":{"@type":"Person","name":"凌虚"},"description":"以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"}</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-VRMQFEVL7J"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VRMQFEVL7J")</script><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"light"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"light"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="凌虚 Blog">凌虚的博客</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/about>作者 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="凌虚 Blog">凌虚的博客</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/about title>作者</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>凌虚</a></span>&nbsp;<span class=post-category>included in <a href=/categories/ai/><i class="far fa-folder fa-fw" aria-hidden=true></i>AI</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-24>2024-10-24</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1459 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#fine-tuning-的基本过程>Fine-Tuning 的基本过程</a></li><li><a href=#基于特定领域模型的-fine-tuning>基于特定领域模型的 Fine-Tuning</a></li><li><a href=#基于-llm-的-fine-tuning-尝试>基于 LLM 的 Fine-Tuning 尝试</a><ul><li><a href=#seq2seq-和-causallm-的区别>Seq2Seq 和 CausalLM 的区别</a></li><li><a href=#llm-fine-tuning>LLM Fine-Tuning</a></li><li><a href=#使用-lora-加速-fine-tuning>使用 LoRA 加速 Fine-Tuning</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></div><div class=content id=content><p>在现在的 <code>AI</code> 领域，<code>Fine-Tuning</code>（微调）是一种常见且有效的方法，通过对已经训练好的模型进行特定任务的微调，可以使模型在特定场景下表现得更加出色和符合需求。在这篇文章中，我将以 <code>Kubernetes</code> 文档的英译中为背景，分享我进行 <code>Fine-Tuning</code> 的探索过程。</p><h2 id=fine-tuning-的基本过程>Fine-Tuning 的基本过程</h2><p><code>Fine-Tuning</code> 的核心思想是，在一个预训练模型的基础上，使用特定领域的数据进行进一步训练，从而让模型更好地适应该领域。通常，<code>Fine-Tuning</code> 包括以下几个步骤：</p><ul><li>选择预训练模型：根据任务需求选择合适的预训练模型，例如特定领域模型或者通用的大语言模型（<code>LLM</code>）。</li><li>准备数据集：收集并整理与目标任务相关的数据集，为模型微调提供训练样本。</li><li>模型微调：通过在该数据集上进行训练，使模型在特定任务中表现得更好。</li><li>模型评估与优化：评估模型表现，根据需要进行调整。</li><li>输出新模型：微调完成后，保存优化后的模型，供后续任务使用。</li></ul><p>在这次实验中，我首先尝试使用专用的翻译模型进行 <code>Fine-Tuning</code>，然后进一步尝试以大语言模型（<code>LLM</code>）为基础进行 <code>Fine-Tuning</code>。</p><h2 id=基于特定领域模型的-fine-tuning>基于特定领域模型的 Fine-Tuning</h2><p>由于翻译是一个特定领域，已经存在很多相关的模型，并且这些模型相比于 <code>LLM</code> 会更小。因此，我首先选择了 <code>HuggingFace</code> 上的 <code>Helsinki-NLP/opus-mt-en-zh</code> 模型进行尝试。</p><p>数据准备：我的数据集取自 Kubernetes 官方文档，然后整理成了 jsonl 文件，内容如下图所示，en 表示英文原文，zh 表示对应的中文翻译：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-dataset.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-dataset.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-dataset.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-dataset.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-dataset.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-dataset.png></p><p>代码实现如下：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code1.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code1.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code1.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code1.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code1.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code1.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code2.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code2.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code2.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code2.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code2.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-marianmt-code2.png></p><p>基本过程就是：加载基础模型、加载并划分数据集、数据预处理、设置训练参数然后训练、评估、最后输出微调好的新模型。</p><p>由于我本地硬件资源的限制，我只能加载部分数据进行训练，并且降低了训练的批次和轮次，读者可以根据实际情况调整为不同的参数。</p><h2 id=基于-llm-的-fine-tuning-尝试>基于 LLM 的 Fine-Tuning 尝试</h2><h3 id=seq2seq-和-causallm-的区别>Seq2Seq 和 CausalLM 的区别</h3><p>上文中使用的翻译领域模型是一种 <code>Seq2Seq</code>（序列到序列）模型，它使用的是 <code>encoder-decoder</code>（编码-解码）架构，编码器将输入序列编码为一个上下文向量，解码器基于这个上下文向量生成输出序列，翻译的过程就是一个编码解码的过程。</p><p>而常见的 <code>LLM</code> 则属于 <code>CausalLM</code>（因果语言模型），基于自回归方式，仅考虑前面的上下文信息来生成后续词语，其实就是文字接龙。</p><h3 id=llm-fine-tuning>LLM Fine-Tuning</h3><p><code>Fine-Tuning</code> 的过程基本类似，但是由于 <code>LLM</code>（<code>CausalLM</code>）与 <code>Seq2Seq</code>（<code>encoder-decoder</code>）模型并不相同，因此两者在数据集的输入格式、训练方式、评估存在区别。</p><p>比如，使用 <code>LLM</code> 进行 <code>Fine-Tuning</code> 的时候，需要对输入数据进行额外的 prompt 格式转换：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;&lt;|im_start|&gt;system
</span></span></span><span class=line><span class=cl><span class=s2>You are a professional translator who can translate English to Chinese accurately while preserving the original formatting and technical terms.
</span></span></span><span class=line><span class=cl><span class=s2>&lt;|im_end|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>&lt;|im_start|&gt;user
</span></span></span><span class=line><span class=cl><span class=s2>Translate the following English text to Chinese:
</span></span></span><span class=line><span class=cl><span class=s2></span><span class=si>{en_text}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>&lt;|im_end|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>&lt;|im_start|&gt;assistant
</span></span></span><span class=line><span class=cl><span class=s2></span><span class=si>{zh_text}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>&lt;|im_end|&gt;&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>通过 system 指定背景，user 代表用户，assistant 代表 AI 的回答，把数据集按照上述格式填充，然后再进行训练。</p><h3 id=使用-lora-加速-fine-tuning>使用 LoRA 加速 Fine-Tuning</h3><p>本地训练 <code>LLM</code> 会更加消耗资源，即使我只选择了 <code>Qwen2.5-0.5B</code> 这个小模型，只加载数据集的部分数据，并且调整训练参数也无法完成 <code>Fine-Tuning</code>。因此，我不得不寻求一种资源占用更低、性能更好的方式，而 <code>LoRA</code> 就是其中一种。</p><p><code>LoRA</code> 通过在训练大型模型时引入低秩矩阵分解，只对模型的一部分参数进行 <code>Fine-Tuning</code>，其余参数保持不变，从而显著减少内存占用并提高训练效率。</p><p>我的代码实现如下：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code1.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code1.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code1.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code1.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code1.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code1.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code2.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code2.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code2.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code2.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code2.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code2.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code3.png data-srcset="https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code3.png, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code3.png 1.5x, https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code3.png 2x" data-sizes=auto alt=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code3.png title=https://raw.githubusercontent.com/RifeWang/images/master/ai/fine-tuning/fine-tuning-llm-lora-code3.png></p><p>通过 <code>LoRA</code> 技术，我得以在有限的硬件资源下完成了 <code>LLM</code> 的 <code>Fine-Tuning</code>。</p><h2 id=总结>总结</h2><p>在这次基于 Kubernetes 文档的翻译实验中，我探索了分别使用特定领域模型和 <code>LLM</code> 进行 <code>Fine-Tuning</code> 的过程，并通过 <code>LoRA</code> 技术有效地提升了 <code>Fine-Tuning</code> 的性能。除了 <code>LoRA</code> 之外，还有 <code>Adapter</code>、<code>QLoRA</code>、<code>DoRA</code> 等等，它们都属于 <code>Parameter-Efficient Fine-Tuning</code>（<code>PEFT</code>）的研究范畴。</p><p>(关注我，无广告，专注技术，不煽动情绪，也欢迎与我交流)</p><hr><p>参考资料：</p><ul><li><em><a href=https://huggingface.co/Helsinki-NLP/opus-mt-en-zh target=_blank rel="noopener noreffer">https://huggingface.co/Helsinki-NLP/opus-mt-en-zh</a></em></li><li><em><a href=https://huggingface.co/Qwen/Qwen2.5-0.5B target=_blank rel="noopener noreffer">https://huggingface.co/Qwen/Qwen2.5-0.5B</a></em></li><li><em><a href=https://arxiv.org/html/2408.13296v1 target=_blank rel="noopener noreffer">https://arxiv.org/html/2408.13296v1</a></em></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2024-10-25</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://rifewang.github.io/fine-tuning-for-translation/ data-title="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调" data-hashtags=AI,LLM><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://rifewang.github.io/fine-tuning-for-translation/ data-hashtag=AI><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://rifewang.github.io/fine-tuning-for-translation/ data-title="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://rifewang.github.io/fine-tuning-for-translation/ data-title="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><i data-svg-src=/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://rifewang.github.io/fine-tuning-for-translation/ data-title="以翻译 Kubernetes 文档为例，探索 AI 模型 Fine-Tuning 微调"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/ai/>AI</a>,&nbsp;<a href=/tags/llm/>LLM</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/k8s-gpu-deviceplugin-cdi-nfd/ class=prev rel=prev title="Kubernetes GPU 调度和 Device Plugin、CDI、NFD、GPU Operator 概述"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Kubernetes GPU 调度和 Device Plugin、CDI、NFD、GPU Operator 概述</a>
<a href=/build-my-chatgpt/ class=next rel=next title="如果自建 ChatGPT，我会如何从 Model、Inference runtime 构建整个系统">如果自建 ChatGPT，我会如何从 Model、Inference runtime 构建整个系统<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2017 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>凌虚</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/katex/katex.min.css><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:50,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>