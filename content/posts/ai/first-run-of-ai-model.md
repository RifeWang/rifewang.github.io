+++
draft = false
date = 2024-10-17T18:30:26+08:00
title = "只想简单跑个 AI 大模型，却发现并不简单"
description = "只想简单跑个 AI 大模型，却发现并不简单"
slug = ""
authors = []
tags = ["AI", "LLM"]
categories = ["AI"]
externalLink = ""
series = []
disableComments = true
+++

之前我用 `Ollama` 在本地跑大语言模型（可以参考`《AI LLM 利器 Ollama 架构和对话处理流程解析》`）。这次想再捣鼓点进阶操作，比如 `fine-tuning`。

我的想法是：既然有现成的大模型，为什么不自己整理些特定领域的数据集，给模型“加点料”呢？这样最后就能得到一个针对特定领域优化过的模型了。

不过，我很快发现，事情并没有想象的那么简单。想要 `fine-tune` ？那得先能通过代码的方式跑起来模型再说吧！于是，这篇文章就诞生了，记录了我如何“简单”跑个 AI 大模型，结果问题不断的过程。

## 云环境还是本地？

众所周知，跑 AI 模型最好有 GPU。好吧，我没有 GPU。那怎么办？没关系！云环境来救场！`Google Colab`、`Kaggle Notebooks` 都挺香的，毕竟谁不喜欢“白嫖”呢？我果断选择了 `Colab`，心想资源丰富又强大。

然而，现实狠狠打了我的脸。空闲 GPU 资源？不存在的！对于免费用户，GPU 资源完全是看缘分，没有就是没有。不过，除此之外，`Colab` 还是很好用的。

![](https://raw.githubusercontent.com/RifeWang/images/master/ai/colab.png)

由于拿不到更好的资源，以及环境存在限制，我还是决定回到我的“战五渣”本地环境。

## Python 环境：折腾得头秃

进入 AI 领域，`Python` 是不可避免的老大哥，但也正是因为它，噩梦开始了。你会遇到各种 `Python` 版本问题，还有管理依赖包的问题。于是你就开始在虚拟环境、各种工具的泥潭中挣扎。`Conda`、`pipenv`、`pipx`、`poetry` 各种工具轮番上场，最后传说中的“现代包管理器” `poetry` 装个 `PyTorch` 都失败了！这让我无比抓狂。

怎么办呢？还是得上 `Docker`！虚拟环境？各种工具？统统扔掉！我只需要一个干净的 `Docker` 环境，把代码目录挂载到容器里，简直不要太爽。

不过，还要处理下容器重启时需要重新下载依赖包的问题，有两种处理办法。第一种是弄镜像，选择一个包含各种依赖环境的大镜像，或者层层构建新的镜像，我嫌麻烦没这么做。第二种就是把依赖包直接持久化，存储到项目目录下（类似于 `node.js` 的 `node_modules`），然后把 `PYTHONPATH` 设置好，指向这些依赖的位置就行了。

![](https://raw.githubusercontent.com/RifeWang/images/master/ai/project-example.png)

再加上 VSCode 的 `Remote Development` 插件，开发环境终于完美拉起！再也不用担心折腾环境问题了，感觉人生都轻松了不少。

## 模型选择：总有一个适合你

环境搞定了，接下来该挑模型了（`Hugging Face` 欢迎你）。兴致勃勃地我决定试试火热的 `LLaMA`，结果 —— 你得申请权限。我以为随便填填表格就行，结果直接被拒绝了！我猜是因为我填的地区不对……

没事，拒绝就拒绝，天涯何处无模型。换个别的！嗯……想想吧，模型种类那么多，挑个生成文本的模型就行。于是，我瞄上了 `Qwen`。`Qwen` 跟 `LLaMA` 都是大家族，下面有多种不同参数的模型，为了不让我的电脑炸掉，最后挑了个小模型 `Qwen/Qwen2.5-0.5B`。

来吧，写好 “Hello, world”，开跑！然后 ………… 小模型也不太行啊，苦等了十几分钟，才给我回消息！不过我也倍感欣慰，毕竟模型是真回消息，速度也比招聘软件的 HR 们快多了。


(关注我，无广告，专注技术，不煽动情绪，也欢迎与我交流)

---

参考资料：

- *https://huggingface.co/*
- *https://huggingface.co/Qwen/Qwen2.5-0.5B*