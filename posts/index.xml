<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on 凌虚 Blog</title><link>https://rifewang.github.io/posts/</link><description>Recent content in Posts on 凌虚 Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 15 Apr 2022 00:00:00 +0800</lastBuildDate><atom:link href="https://rifewang.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Elasticsearch 向量搜索</title><link>https://rifewang.github.io/posts/elasticsearch/es-vector-search/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-vector-search/</guid><description>Elasticsearch 向量搜索 Link to heading 本文将会介绍 Elasticsearch 向量搜索的两种方式。
向量搜索 Link to heading 提到向量搜索，我想你一定想知道：
向量搜索是什么？ 向量搜索的应用场景有哪些？ 向量搜索与全文搜索有何不同？ ES 的全文搜索简而言之就是将文本进行分词，然后基于词通过 BM25 算法计算相关性得分，从而找到与搜索语句相似的文本，其本质上是一种 term-based（基于词）的搜索。
全文搜索的实际使用已经非常广泛，核心技术也非常成熟。但是，除了文本内容之外，现实生活中还有非常多其它的数据形式，例如：图片、音频、视频等等，我们能不能也对这些数据进行搜索呢？
答案是 Yes !
随着机器学习和人工智能等技术的发展，万物皆可 Embedding。换句话说就是，我们可以对文本、图片、音频、视频等等一切数据通过 Embedding 相关技术将其转换成特征向量，而一旦向量有了，向量搜索的需求随之也越发强烈，向量搜索的应用场景也变得一望无际、充满想象力。
ES 向量搜索说明 Link to heading ES 向量搜索目前有两种方式:
script_score _knn_search script_score 精确搜索 Link to heading ES 7.6 版本对新增的字段类型 dense_vector 确认了稳定性保证，这个字段类型就是用来表示向量数据的。
数据建模示例：
PUT my-index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_vector&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;dense_vector&amp;#34;, &amp;#34;dims&amp;#34;: 128 }, &amp;#34;my_text&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;keyword&amp;#34; } } } } 如上图所示，我们在索引中建立了一个 dims 维度为 128 的向量数据字段。</description></item><item><title>Terraform: 基础设施即代码</title><link>https://rifewang.github.io/posts/devops/terraform-overview/</link><pubDate>Sun, 27 Mar 2022 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/devops/terraform-overview/</guid><description>Terraform: 基础设施即代码 Link to heading 问题 Link to heading 现如今有很多 IT 系统的基础设施直接使用了云厂商提供的服务，假设我们需要构建以下基础设施：
VPC 网络 虚拟主机 负载均衡器 数据库 文件存储 &amp;hellip; 那么在公有云的环境中，我们一般怎么做？
在云厂商提供的前端管理页面上手动操作吗？
这也太费劲了吧，尤其是当基础设施越来越多、越来越复杂、以及跨多个云环境的时候，这些基础设施的配置和管理便会碰到一个巨大的挑战。
Terraform Link to heading 为了解决上述问题，Terrafrom 应运而生。
使用 Terraform ，我们只需要编写简单的声明式代码，形如：
... resource &amp;#34;alicloud_db_instance&amp;#34; &amp;#34;instance&amp;#34; { engine = &amp;#34;MySQL&amp;#34; engine_version = &amp;#34;5.6&amp;#34; instance_type = &amp;#34;rds.mysql.s1.small&amp;#34; instance_storage = &amp;#34;10&amp;#34; ... } 然后执行几个简单的 terraform 命令便可以轻松创建一个阿里云的数据库实例。
这就是 Infrastructure as code 基础设施即代码。也就是通过代码而不是手动流程来管理和配置基础设施。
正如其官方文档所述，与手动管理基础设施相比，使用 Terraform 有以下几个优势：
Terraform 可以轻松管理多个云平台上的基础设施。 使用人类可读的声明式的配置语言，有助于快速编写基础设施代码。 Terraform 的状态允许您在整个部署过程中跟踪资源更改。 可以对这些基础设施代码进行版本控制，从而安全地进行协作。 Provider &amp;amp; Module Link to heading 你也许会感到困惑，我只是简单的应用了所写的声明式代码，怎么就构建出来了基础设施，这中间发生了什么？</description></item><item><title>加速 Kubernetes 镜像拉取</title><link>https://rifewang.github.io/posts/kubernetes/speed-up-image-pull/</link><pubDate>Sun, 13 Mar 2022 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/kubernetes/speed-up-image-pull/</guid><description>加速 Kubernetes 镜像拉取 Link to heading Kubernetes pod 启动时会拉取用户指定的镜像，一旦这个过程耗时太久就会导致 pod 长时间处于 pending 的状态，从而无法快速提供服务。
镜像拉取的过程参考下图所示：
Pod 的 imagePullPolicy 镜像拉取策略有三种：
IfNotPresent：只有当镜像在本地不存在时才会拉取。 Always：kubelet 会对比镜像的 digest ，如果本地已缓存则直接使用本地缓存，否则从镜像仓库中拉取。 Never：只使用本地镜像，如果不存在则直接失败。 说明：每个镜像的 digest 一定唯一，但是 tag 可以被覆盖。
从镜像拉取的过程来看，我们可以从以下三个方面来加速镜像拉取：
缩减镜像大小： 使用较小的基础镜像、移除无用的依赖、减少镜像 layer 、使用多阶段构建等等。 推荐使用 docker-slim 加快镜像仓库与 k8s 节点之间的网络传输速度。 主动缓存镜像： Pre-pulled 预拉取镜像，以便后续直接使用本地缓存，比如可以使用 daemonset 定期同步仓库中的镜像到 k8s 节点本地。 题外话 1：本地镜像缓存多久？是否会造成磁盘占用问题？
本地缓存的镜像一定会占用节点的磁盘空间，也就是说缓存的镜像越多，占用的磁盘空间越大，并且缓存的镜像默认一直存在，并没有 TTL 机制（比如说多长时间以后自动过期删除）。
但是，k8s 的 GC 机制会自动清理掉镜像。当节点的磁盘使用率达到 HighThresholdPercent 高百分比阈值时（默认 85% ）会触发垃圾回收，此时 kubelet 会根据使用情况删除最旧的不再使用的镜像，直到磁盘使用率达到 LowThresholdPercent（默认 80% ）。
题外话 2：镜像 layer 层数真的越少越好吗？
我们经常会看到一些文章说在 Dockerfile 里使用更少的 RUN 命令之类的减少镜像的 layer 层数然后缩减镜像的大小，layer 越少镜像越小这确实没错，但是某些场景下得不偿失。首先，如果你的 RUN 命令很大，一旦你修改了其中某一个小的部分，那么这个 layer 在构建的时候就只能重新再来，无法使用任何缓存；其次，镜像的 layer 在上传和下载的过程中是可以并发的，而单独一个大的层无法进行并发传输。</description></item><item><title>web 安全系列文章【译文】</title><link>https://rifewang.github.io/posts/linkto/web-security/</link><pubDate>Thu, 12 Aug 2021 10:58:20 +0800</pubDate><guid>https://rifewang.github.io/posts/linkto/web-security/</guid><description> Cross-site request forgery (CSRF) Link to heading CSRF XSS vs CSRF CSRF tokens SameSite cookies Clickjacking (UI redressing) Link to heading Clickjacking (UI redressing) Cross-origin resource sharing (CORS) Link to heading CORS Same-origin policy (SOP) Access-control-allow-origin Server-side request forgery (SSRF) Link to heading Server-side request forgery (SSRF) Blind SSRF vulnerabilities HTTP request smuggling Link to heading HTTP request smuggling Finding HTTP request smuggling vulnerabilities Exploiting HTTP request smuggling vulnerabilities OS command injectionn Link to heading OS command injection Directory traversal Link to heading Directory traversal DOM-based vulnerabilities Link to heading DOM-based vulnerabilities DOM clobbering HTTP Host header attacks Link to heading HTTP Host header attacks Exploiting HTTP Host header vulnerabilities Password reset poisoning</description></item><item><title>解读 MySQL Client/Server Protocol: Connection &amp; Replication</title><link>https://rifewang.github.io/posts/mysql/protocol-connectionreplication/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/mysql/protocol-connectionreplication/</guid><description>解读 MySQL Client/Server Protocol: Connection &amp;amp; Replication Link to heading MySQL 客户端与服务器之间的通信基于特定的 TCP 协议，本文将会详解其中的 Connection 和 Replication 部分，这两个部分分别对应的是客户端与服务器建立连接、完成认证鉴权，以及客户端注册成为一个 slave 并获取 master 的 binlog 日志。
Connetcion Phase Link to heading MySQL 客户端想要与服务器进行通信，第一步就是需要成功建立连接，整个过程如下图所示：
client 发起一个 TCP 连接。 server 响应一个 Initial Handshake Packet（初始化握手包），内容会包含一个默认的认证方式。 这一步是可选的，双方建立 SSL 加密连接。 client 回应 Handshake Response Packet，内容需要包括用户名和按照指定方式进行加密后的密码数据。 server 响应 OK_Packet 确认认证成功，或者 ERR_Packet 表示认证失败并关闭连接。 Packet Link to heading 一个 Packet 其实就是一个 TCP 包，所有包都有一个最基本的结构：
如上图所示，所有包都可以看作由 header 和 body 两部分构成：第一部分 header 总共有 4 个字节，3 个字节用来标识 body 即 payload 的大小，1 个字节记录 sequence ID；第二部分 body 就是 payload 实际的负载数据。</description></item><item><title>同步 MySQL 数据至 Elasticsearch/Redis/MQ 等的五种方式</title><link>https://rifewang.github.io/posts/mysql/sync-data-from-mysql/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/mysql/sync-data-from-mysql/</guid><description>同步 MySQL 数据至 Elasticsearch/Redis/MQ 等的五种方式 Link to heading 在实际应用中，我们经常需要把 MySQL 的数据同步至其它数据源，也就是在对 MySQL 的数据进行了新增、修改、删除等操作后，把该数据相关的业务逻辑变更也应用到其它数据源，例如：
MySQL -&amp;gt; Elasticsearch ，同步 ES 的索引 MySQL -&amp;gt; Redis ，刷新缓存 MySQL -&amp;gt; MQ (如 Kafka 等) ，投递消息 本文总结了五种数据同步的方式。
1. 业务层同步 Link to heading 由于对 MySQL 数据的操作也是在业务层完成的，所以在业务层同步操作另外的数据源也是很自然的，比较常见的做法就是在 ORM 的 hooks 钩子里编写相关同步代码。
这种方式的缺点是，当服务越来越多时，同步的部分可能会过于分散从而导致难以更新迭代，例如对 ES 索引进行不兼容迁移时就可能会牵一发而动全身。
2. 中间件同步 Link to heading 当应用架构演变为微服务时，各个服务里可能不再直接调用 MySQL ，而是通过一层 middleware 中间件，这时候就可以在中间件操作 MySQL 的同时同步其它数据源。
这种方式需要中间件去适配，具有一定复杂度。
3. 定时任务根据 updated_at 字段同步 Link to heading 在 MySQL 的表结构里设置特殊的字段，如 updated_at（数据的更新时间），根据此字段，由定时任务去查询实际变更的数据，从而实现数据的增量更新。</description></item><item><title>Elasticsearch 分布式搜索的运行机制</title><link>https://rifewang.github.io/posts/elasticsearch/es-distribute-search-steps/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-distribute-search-steps/</guid><description>Elasticsearch 分布式搜索的运行机制 Link to heading ES 有两种 search_type 即搜索类型：
query_then_fetch （默认） dfs_query_then_fetch query_then_fetch Link to heading 用户发起搜索，请求到集群中的某个节点。 query 会被发送到所有相关的 shard 分片上。 每个 shard 分片独立执行 query 搜索文档并进行排序分页等，打分时使用的是分片本身的 Local Term/Document 频率。 分片的 query 结果（只有元数据，例如 _id 和 _score）返回给请求节点。 请求节点对所有分片的 query 结果进行汇总，然后根据打分排序和分页，最后选择出搜索结果文档（也只有元数据）。 根据元数据去对应的 shard 分片拉取存储在磁盘上的文档的详细数据。 得到详细的文档数据，组成搜索结果，将结果返回给用户。 缺点：由于每个分片独立使用自身的而不是全局的 Term/Document 频率进行相关度打分，当数据分布不均匀时可能会造成打分偏差，从而影响最终搜索结果的相关性。
dfs_query_then_fetch Link to heading dfs_query_then_fetch 与 query_then_fetch 的运行机制非常类似，但是有两点不同。
用户发起搜索，请求到集群中的某个节点。 预查询每个分片，得到全局的 Global Term/Document 频率。 query 会被发送到所有相关的 shard 分片上。 每个 shard 分片独立执行 query 搜索文档并进行排序分页等，打分时使用的是分片本身的 Global Term/Document 频率。 分片的 query 结果（只有元数据，例如 _id 和 _score）返回给请求节点。 请求节点对所有分片的 query 结果进行汇总，然后根据打分排序和分页，最后选择出搜索结果文档（也只有元数据）。 根据元数据去对应的 shard 分片拉取存储在磁盘上的文档的详细数据。 得到详细的文档数据，组成搜索结果，将结果返回给用户。 缺点：太耗费资源，一般还是不建议使用。</description></item><item><title>Elasticsearch Search Template</title><link>https://rifewang.github.io/posts/elasticsearch/es-search-template/</link><pubDate>Mon, 16 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-search-template/</guid><description>Elasticsearch Search Template Link to heading 所谓 search template 搜索模板其实就是：
预先定义好查询语句 DSL 的结构并预留参数 搜索的时再传入参数值 渲染出完整的 DSL ，最后进行搜索 使用搜索模板可以将 DSL 从应用程序中解耦出来，并且可以更加灵活的更改查询语句。
例如：
GET _search/template { &amp;#34;source&amp;#34; : { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34; : { &amp;#34;{{my_field}}&amp;#34; : &amp;#34;{{my_value}}&amp;#34; } } }, &amp;#34;params&amp;#34; : { &amp;#34;my_field&amp;#34; : &amp;#34;message&amp;#34;, &amp;#34;my_value&amp;#34; : &amp;#34;foo&amp;#34; } } 构造出来的 DSL 就是：
{ &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;message&amp;#34;: &amp;#34;foo&amp;#34; } } } 在模板中通过 {{ }} 的方式预留参数，然后查询时再指定对应的参数值，最后填充成具体的查询语句进行搜索。
搜索模板 API Link to heading 为了实现搜索模板和查询分离，我们首先需要单独保存和管理搜索模板。</description></item><item><title>构造请求日志分析系统</title><link>https://rifewang.github.io/posts/elasticsearch/log-analyzer-system/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/log-analyzer-system/</guid><description>构造请求日志分析系统 Link to heading 请求日志记录哪些数据 Link to heading time_local : 请求的时间 remote_addr : 客户端的 IP 地址 request_method : 请求方法 request_schema : 请求协议，常见的 http 和 https request_host : 请求的域名 request_path : 请求的 path 路径 request_query : 请求的 query 参数 request_size : 请求的大小 referer : 请求来源地址，假设你在 a.com 网站下贴了 b.com 的链接，那么当用户从 a.com 点击访问 b.com 的时候，referer 记录的就是 a.com ，这个是浏览器的行为 user_agent : 客户端浏览器相关信息 status : 请求的响应状态 request_time : 请求的耗时 bytes_sent : 响应的大小 很多时候我们会使用负载网关去代理转发请求给实际的后端服务，这时候请求日志还会包括以下数据：
upstream_host : 代理转发的 host upstream_addr : 代理转发的 IP 地址 upstream_url : 代理转发给服务的 url upstream_status : 上游服务返回的 status proxy_time : 代理转发过程中的耗时 数据衍生 Link to heading 客户端 IP 地址可以衍生出以下数据：</description></item><item><title>Elasticsearch 自定义打分 Function score query</title><link>https://rifewang.github.io/posts/elasticsearch/es-function-score-query/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/es-function-score-query/</guid><description>Elasticsearch 自定义打分 Function score query Link to heading Elasticsearch 会为 query 的每个文档计算一个相关度得分 score ，并默认按照 score 从高到低的顺序返回搜索结果。 在很多场景下，我们不仅需要搜索到匹配的结果，还需要能够按照某种方式对搜索结果重新打分排序。例如：
搜索具有某个关键词的文档，同时考虑到文档的时效性进行综合排序。 搜索某个旅游景点附近的酒店，同时根据距离远近和价格等因素综合排序。 搜索标题包含 elasticsearch 的文章，同时根据浏览次数和点赞数进行综合排序。 Function score query 就可以让我们实现对最终 score 的自定义打分。
score 自定义打分过程 Link to heading 为了行文方便，本文把 ES 对 query 匹配的文档进行打分得到的 score 记为 query_score ，而最终搜索结果的 score 记为 result_score ，显然，一般情况下（也就是不使用自定义打分时），result_score 就是 query_score 。
那么当我们使用了自定义打分之后呢？最终结果的 score 即 result_score 的计算过程如下：
跟原来一样执行 query 并且得到原来的 query_score 。 执行设置的自定义打分函数，并为每个文档得到一个新的分数，本文记为 func_score 。 最终结果的分数 result_score 等于 query_score 与 func_score 按某种方式计算的结果（默认是相乘）。 例如，搜索标题包含 elasticsearch 的文档。</description></item><item><title>Logstash 入门</title><link>https://rifewang.github.io/posts/elasticsearch/logstash/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/elasticsearch/logstash/</guid><description>Logstash 入门 Link to heading Logstash 是什么 Link to heading Logstash 就是一个开源的数据流工具，它会做三件事：
从数据源拉取数据 对数据进行过滤、转换等处理 将处理后的数据写入目标地 例如：
监听某个目录下的日志文件，读取文件内容，处理数据，写入 influxdb 。 从 kafka 中消费消息，处理数据，写入 elasticsearch 。 为什么要用 Logstash ？ Link to heading 方便省事。
假设你需要从 kafka 中消费数据，然后写入 elasticsearch ，如果自己编码，你得去对接 kafka 和 elasticsearch 的 API 吧，如果你用 Logstash ，这部分就不用自己去实现了，因为 Logstash 已经为你封装了对应的 plugin 插件，你只需要写一个配置文件形如：
input { kafka { # kafka consumer 配置 } } filter { # 数据处理配置 } output { elasticsearch { # elasticsearch 输出配置 } } 然后运行 logstash 就可以了。</description></item><item><title>又拍图片管家亿级图像之搜图系统的两代演进及底层原理</title><link>https://rifewang.github.io/posts/engineering/image-search-total/</link><pubDate>Thu, 04 Jun 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/engineering/image-search-total/</guid><description>又拍图片管家亿级图像之搜图系统的两代演进及底层原理 Link to heading 前言 Link to heading 又拍图片管家当前服务了千万级用户，管理了百亿级图片。当用户的图库变得越来越庞大时，业务上急切的需要一种方案能够快速定位图像，即直接输入图像，然后根据输入的图像内容来找到图库中的原图及相似图，而以图搜图服务就是为了解决这个问题。
本人于在职期间独立负责并实施了整个以图搜图系统从技术调研、到设计验证、以及最后工程实现的全过程。而整个以图搜图服务也是经历了两次的整体演进：从 2019 年初开始第一次技术调研，经历春节假期，2019 年 3、4 月份第一代系统整体上线；2020 年初着手升级方案调研，经历春节及疫情，2020 年 4 月份开始第二代系统的整体升级。
本文将会简述两代搜图系统背后的技术选型及基本原理。
基础概要 Link to heading 图像是什么？ Link to heading 与图像打交道，我们必须要先知道：图像是什么？
答案：像素点的集合。
比如：
左图红色圈中的部分其实就是右图中一系列的像素点。
再举例：
假设上图红色圈的部分是一幅图像，其中每一个独立的小方格就是一个像素点（简称像素），像素是最基本的信息单元，而这幅图像的大小就是 11 x 11 px 。
图像的数学表示 Link to heading 每个图像都可以很自然的用矩阵来表示，每个像素点对应的就是矩阵中的一个元素。
二值图像 Link to heading 二值图像的像素点只有黑白两种情况，因此每个像素点可以由 0 和 1 来表示。
比如一张 4 * 4 二值图像：
0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 RGB 图像 Link to heading 红（Red）、绿（Green）、蓝（Blue）作为三原色可以调和成任意的颜色，对于 RGB 图像，每个像素点包含 RGB 共三个通道的基本信息，类似的，如果每个通道用 8 bit 表示即 256 级灰度，那么一个像素点可以表示为：</description></item><item><title>以图搜图系统工程实践</title><link>https://rifewang.github.io/posts/engineering/image-search-system2/</link><pubDate>Sat, 11 Apr 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/engineering/image-search-system2/</guid><description>以图搜图系统工程实践 Link to heading 之前写过一篇概述: 以图搜图系统概述 。
以图搜图系统需要解决的主要问题是：
提取图像特征向量（用特征向量去表示一幅图像） 特征向量的相似度计算（寻找内容相似的图像） 对应的工程实践，具体为：
卷积神经网络 CNN 提取图像特征 向量搜索引擎 Milvus CNN Link to heading 使用卷积神经网路 CNN 去提取图像特征是一种主流的方案，具体的模型则可以使用 VGG16 ，技术实现上则使用 Keras + TensorFlow ，参考 Keras 官方示例：
from keras.applications.vgg16 import VGG16 from keras.preprocessing import image from keras.applications.vgg16 import preprocess_input import numpy as np model = VGG16(weights=&amp;#39;imagenet&amp;#39;, include_top=False) img_path = &amp;#39;elephant.jpg&amp;#39; img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.</description></item><item><title>以图搜图系统概述</title><link>https://rifewang.github.io/posts/engineering/image-search-system/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/engineering/image-search-system/</guid><description>以图搜图系统概述 Link to heading 以图搜图指的是根据图像内容搜索出相似内容的图像。
构建一个以图搜图系统需要解决两个最关键的问题：首先，提取图像特征；其次，特征数据搜索引擎，即特征数据构建成数据库并提供相似性搜索的功能。
图像特征表示 Link to heading 介绍三种方式。
图像哈希 Link to heading 图像通过一系列的变换和处理最终得到的一组哈希值称之为图像的哈希值，而中间的变换和处理过程则称之为哈希算法。
图像的哈希值是对这张图像的整体抽象表示。
比如 Average Hash 算法的计算过程： Reduce size : 将原图压缩到 8 x 8 即 64 像素大小，忽略细节。 Reduce color : 灰度处理得到 64 级灰度图像。 Average the colors : 计算 64 级灰度均值。 Compute the bits : 二值化处理，将每个像素与上一步均值比较并分别记为 0 或者 1 。 Construct the hash : 根据上一步结果矩阵构成一个 64 bit 整数，比如按照从左到右、从上到下的顺序。最后得到的就是图像的均值哈希值。 参考：http://www.hackerfactor.com/blog/?/archives/432-Looks-Like-It.html
图像哈希算法有很多种，包含但不限于:
AverageHash : 也叫 Different Hash PHash : Perceptual Hash MarrHildrethHash : Marr-Hildreth Operator Based Hash RadialVarianceHash : Image hash based on Radon transform BlockMeanHash : Image hash based on block mean ColorMomentHash : Image hash based on color moments 我们最常见可能就是 PHash 。</description></item><item><title>GitHub Actions 指南</title><link>https://rifewang.github.io/posts/uncate/github-actions/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/github-actions/</guid><description>GitHub Actions 指南 Link to heading GitHub Actions 使你可以直接在你的 GitHub 库中创建自定义的工作流，工作流指的就是自动化的流程，比如构建、测试、打包、发布、部署等等，也就是说你可以直接进行 CI（持续集成）和 CD （持续部署）。
基本概念 Link to heading workflow : 一个 workflow 工作流就是一个完整的过程，每个 workflow 包含一组 jobs 任务。 job : jobs 任务包含一个或多个 job ，每个 job 包含一系列的 steps 步骤。 step : 每个 step 步骤可以执行指令或者使用一个 action 动作。 action : 每个 action 动作就是一个通用的基本单元。 配置 workflow Link to heading workflow 必须存储在你的项目库根路径下的 .github/workflows 目录中，每一个 workflow 对应一个具体的 .yml 文件（或者 .yaml）。
workflow 示例：
name: Greet Everyone # This workflow is triggered on pushes to the repository.</description></item><item><title>给你的库加上酷炫的小徽章</title><link>https://rifewang.github.io/posts/uncate/ava-codecov-travis/</link><pubDate>Sat, 21 Dec 2019 13:36:00 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/ava-codecov-travis/</guid><description>给库加上酷炫的小徽章 &amp;amp; ava、codecov、travis 示例 Link to heading GitHub 很多开源库都会有几个酷炫的小徽章，比如：
这些是怎么加上去的呢？
Shields.io Link to heading 首先这些徽章可以直接去 shields.io 网站自动生成。
比如：
就是 version 这一类里的一种图标，选择 npm 一栏填入包名，然后复制成 Markdown 内容，就会得到诸如：
![npm (tag)](https://img.shields.io/npm/v/io-memcached/latest) 直接粘贴在 .md 文件中就可以使用了，最后展现的就是这个图标。
当然还有其他很多徽章都任由你挑选，不过某些徽章是需要额外进行一些配置，比如这里的 (自动构建通过) 和 (测试覆盖率)。
AVA Link to heading 谈到测试覆盖率必须先有单元测试，本文使用 ava 作为示例，ava 是一个 js 测试库，强烈推荐你使用它。
1、安装
npm init ava 2、使用示例
编写 test.js 文件：
import test from &amp;#39;ava&amp;#39; import Memcached from &amp;#39;../lib/memcached&amp;#39;; test.before(t =&amp;gt; { const memcached = new Memcached([&amp;#39;127.0.0.1:11211&amp;#39;], { pool: { max: 2, min: 0 }, timeout: 5000 }); t.</description></item><item><title>使用 Makefile 构建指令集</title><link>https://rifewang.github.io/posts/uncate/makefile/</link><pubDate>Sun, 15 Dec 2019 13:39:47 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/makefile/</guid><description>使用 Makefile 构建指令集 Link to heading make 是一个历史悠久的构建工具，通过配置 Makefile 文件就可以很方便的使用你自己自定义的各种指令集，且与具体的编程语言无关。 例如配置如下的 Makefile :
run dev: NODE_ENV=development nodemon server.js 这样当你在命令行执行 make run dev 时其实就会执行 NODE_ENV=development nodemon server.js 指令。
使用 Makefile 构建指令集可以很大的提升工作效率。
Makefile 基本语法 Link to heading &amp;lt;target&amp;gt;: &amp;lt;prerequisites&amp;gt; &amp;lt;commands&amp;gt; target 其实就是执行的目标，prerequisites 是执行这条指令的前置条件，commands 就是具体的指令内容。
示例：
build: clean go build -o myapp main.go clean: rm -rf myapp 这里的 build 有一个前置条件 clean ，意思就是当你执行 make build 时，会先执行 clean 的指令内容 rm -rf myapp ，然后再执行 build 的内容 go build -o myapp main.</description></item><item><title>实现 memcached 客户端：TCP、连接池、一致性哈希、自定义协议</title><link>https://rifewang.github.io/posts/uncate/create-memcached-client/</link><pubDate>Mon, 09 Dec 2019 13:41:38 +0800</pubDate><guid>https://rifewang.github.io/posts/uncate/create-memcached-client/</guid><description>实现 memcached 客户端：TCP、连接池、一致性哈希、自定义协议。 Link to heading 废话不多说，文本将带你实现一个简单的 memcached 客户端。
集群：一致性哈希 Link to heading memcached 本身并不支持集群，为了使用集群，我们可以自己在客户端实现路由分发，将相同的 key 路由到同一台 memcached 上去即可。 路由算法有很多，这里我们使用一致性哈希算法。
一致性哈希算法的原理：
一致性哈希算法已经有开源库 hashring 实现，基本用法：
const HashRing = require(&amp;#39;hashring&amp;#39;); // 输入集群地址构造 hash ring const ring = new HashRing([&amp;#39;127.0.0.1:11211&amp;#39;, &amp;#39;127.0.0.2:11211&amp;#39;]); // 输入 key 获取指定节点 const host = ring.get(key); TCP 编程 Link to heading 包括 memcached 在内的许多系统对外都是通过 TCP 通信。在 Node.js 中建立一个 TCP 连接并进行数据的收发很简单：
const net = require(&amp;#39;net&amp;#39;); const socket = new net.Socket(); socket.</description></item><item><title>时序数据库 InfluxDB（七）</title><link>https://rifewang.github.io/posts/influxdb/7/</link><pubDate>Sun, 17 Nov 2019 13:43:48 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/7/</guid><description> 单点故障和容灾备份 Link to heading InfluxDB 开源的社区版本面临的最大的问题就是单点故障和容灾备份，有没有一个简单的方案去解决这个问题呢？
既然有单点故障的可能，那么索性写入多个节点，同时也解决了容灾备份的问题：
1、在不同的机器上配置多个 InfluxDB 实例，写入数据时，直接由客户端并发写入多个实例。（为什么不用代理，因为代理自身就是个单点）。
2、当某个 InfluxDB 实例故障而导致写入失败时，记录失败的数据和节点，这些失败的数据可以临时存储在数据库、消息中间件、日志文件等等里面。
3、通过自定义的 worker 拉取上一步记录的失败的数据然后重写这些数据。
4、多个 InfluxDB 中的数据最终一致。
当然你需要注意的是：
1、由于是并发写入多个节点，且不同机器的状况不一，所以写入数据应该设置一个超时时间。
2、写入失败的数据必须要与节点相对应，同时你应该考虑如何去定义失败的数据：由于格式不正确或者权限问题导致的 4xx 或者 InfluxDB 本身异常导致的 5xx ，这些与 InfluxDB 宕机等故障导致的失败显然是不同的。
3、由于失败的数据需要临时存储在一个数据容器中，你应该考虑所使用的数据容器能否承载故障期间写入的数据压力，以及如果数据要求不可丢失，那么数据容器也需要有对应的支持。
4、失败数据的重写是一个异步的过程，所以写入的数据应该由客户端指定明确的时间戳，而不是使用 InfluxDB 写入时默认生成的时间戳。
5、故障期间多个 InfluxDB 可能存在数据不一致的情况。
相关文章：
时序数据库 InfluxDB（一） 时序数据库 InfluxDB（二） 时序数据库 InfluxDB（三） 时序数据库 InfluxDB（四） 时序数据库 InfluxDB（五） 时序数据库 InfluxDB（六） 时序数据库 InfluxDB（七）</description></item><item><title>时序数据库 InfluxDB（六）</title><link>https://rifewang.github.io/posts/influxdb/6/</link><pubDate>Wed, 06 Nov 2019 13:36:39 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/6/</guid><description>CQ 连续查询 Link to heading 连续查询 Continuous Queries（ CQ ）是 InfluxDB 很重要的一项功能，它的作用是在 InfluxDB 数据库内部自动定期的执行查询，然后将查询结果存储到指定的 measurement 里。
配置文件中的相关配置：
[continuous_queries] enabled = true log-enabled = true query-stats-enabled = false run-interval = &amp;#34;1s&amp;#34; enabled = true ：开启CQ log-enabled = true ：输出 CQ 日志 query-stats-enabled = false ：关闭 CQ 执行相关的监控，不会将统计数据写入默认的监控数据库 _internal run-interval = &amp;ldquo;1s&amp;rdquo; ：InfluxDB 每隔 1s 检查是否有 CQ 需要执行 基本语法 Link to heading 一 、 Link to heading 基本语法：
CREATE CONTINUOUS QUERY &amp;lt;cq_name&amp;gt; ON &amp;lt;database_name&amp;gt; BEGIN &amp;lt;cq_query&amp;gt; END 在某个数据库上创建一个 CQ ，而查询的具体内容 cq_query 的语法为：</description></item><item><title>时序数据库 InfluxDB（五）</title><link>https://rifewang.github.io/posts/influxdb/5/</link><pubDate>Wed, 30 Oct 2019 13:33:30 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/5/</guid><description>系统监控 Link to heading InfluxDB 自带有一个监控系统，默认情况下此功能是开启的，每隔 10 秒中采集一次系统数据并把数据写入到 _internal 数据库中，其默认使用名称为 monitor 的 RP（数据保留 7 天），相关配置见配置文件中的：
[monitor] store-enabled = true store-database = &amp;#34;_internal&amp;#34; store-interval = &amp;#34;10s&amp;#34; _internal 数据库与其它数据库的使用方式完全一致，其记录的统计数据分为多个 measurements ：
cq ：连续查询 database ：数据库 httpd ：HTTP 相关 queryExecutor ：查询执行器 runtime ：运行时 shard ：分片 subscriber ：订阅者 tsm1_cache ：TSM cache 缓存 tsm1_engine ：TSM 引擎 tsm1_filestore ：TSM filestore tsm1_wal ：TSM 预写日志 write ：数据写入 比如查询最近一次统计的数据写入情况：
select * from &amp;#34;write&amp;#34; order by time desc limit 1 _internal 数据库里的这些 measurements 中具体有哪些 field ，每个 field 数据又代表了什么含义，请参考官方文档：</description></item><item><title>时序数据库 InfluxDB（四）</title><link>https://rifewang.github.io/posts/influxdb/4/</link><pubDate>Mon, 28 Oct 2019 13:27:36 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/4/</guid><description>存储引擎 Link to heading InfluxDB 数据的写入如下图所示：
所有数据先写入到 WAL（ Write Ahead Log ）预写日志文件，并同步到 Cache 缓存中，当 Cache 缓存的数据达到了一定的大小，或者达到一定的时间间隔之后，数据会被写入到 TSM 文件中。
为了更高效的存储大量数据，存储引擎会将数据进行压缩处理，压缩的输入和输出都是 TSM 文件，因此为了以原子方式替换以及删除 TSM 文件，存储引擎由 FileStore 负责调节对所有 TSM 文件的访问权限。
Compaction Planner 负责确定哪些 TSM 文件已经准备好了可以进行压缩，并确保多个并发压缩不会互相干扰。
Compactor 压缩器则负责具体的 Compression 压缩工作。
为了处理文件，存储引擎通过 Writers/Readers 处理数据的写和读。另外存储引擎还会使用 In-Memory Index 内存索引快速访问 measurements、tags、series 等数据。
存储引擎的组成部分：
In-Memory Index ：跨分片的共享内存索引，并不是存储引擎本身特有的，存储引擎只是用到了它。 WAL ：预写日志。 Cache ：同步缓存 WAL 的内容，并最终刷写到 TSM 文件中去。 TSM Files ：特定格式存储最终数据的磁盘文件。 FileStore ：调节对磁盘上所有TSM文件的访问。 Compactor ：压缩器。 Compaction Planner ：压缩计划。 Compression ：编码解码压缩。 Writers/Readers ：读写文件。 硬件指南 Link to heading 为了应对不同的负载情况，我需要机器具有怎样的硬件配置？</description></item><item><title>时序数据库 InfluxDB（三）</title><link>https://rifewang.github.io/posts/influxdb/3/</link><pubDate>Sun, 27 Oct 2019 13:25:48 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/3/</guid><description>数据类型 Link to heading InfluxDB 是一个无结构模式，这也就是说你无需事先定义好表以及表的数据结构。
InfluxDB 支持的数据类型非常简单：
measurement : string tag key : string tag value : string field key : string field value : string , float , interger , boolean 你可以看到除了 field value 支持的数据类型多一点之外，其余全是字符串类型。
当然还有最重要的 timestamp ，InfluxDB 中的时间都是 UTC 时间，而且时间精度非常高，默认为纳秒。
数据结构设计 Link to heading 在实际使用中，数据都是存储在 tag 或者 field 中，这两者最重要的区别就是，tag 会构建索引（也就是说查询时，where 条件里的是 tag ，则查询性能更高），field 则不会被索引。
存储数据到底是使用 tag 还是 field ，参考以下原则：
常用于查询条件的数据存储为 tag 。 计划使用 GROUP BY() 的数据存储为 tag 。 计划使用 InfluxQL function 的数据存储为 field 。 数据不只是 string 类型的存储为 field 。 对于标识性的名称，如 database、RP、user、measurement、tag key、field key 这些应该避免使用 InfluxQL 中的关键字。</description></item><item><title>时序数据库 InfluxDB（二）</title><link>https://rifewang.github.io/posts/influxdb/2/</link><pubDate>Sat, 26 Oct 2019 13:14:35 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/2/</guid><description>RP Link to heading 先回顾一下 RP 策略（ retention policy ），它由三个部分构成：
DURATION：数据的保留时长。 REPLICATION：集群模式下数据的副本数，单节点无效。 SHARD DURATION：可选项，shard group 划分的时间范围。 前两个部分没啥好说的，而 shard duration 和 shard group 的概念你可能会感到比较陌生。
shard 是什么？
先来看数据的层次结构：
如果所示，一个 database 对应一个实际的磁盘上的文件夹，该数据库下不同的 RP 策略对应不同的文件夹。
shard group 只是一个逻辑概念，并没有实际的磁盘文件夹，shard group 包含有一个或多个 shard 。
最终的数据是存储在 shard 中的，每个 shard 也对应一个具体的磁盘文件目录，数据是按照时间范围分割存储的，shard duration 也就是划分 shard group 的时间范围（例如 shard duration 如果是一周，那么第一周的数据就会存储到一个 shard group 中，第二周的数据会存储到另外一个 shard group 中，以此类推）。
另外，每个 shard 目录下都有一个 TSM 文件（后缀名为 .tsm ），正是这个文件存储了最后编码和压缩后的数据。shard group 下的 shard 是按照 series 来划分的，每个 shard 包含一组特定的 series ，换句话说特定 shard group 中的特定 series 上的所有 points 点都存储在同一个 TSM 文件中。</description></item><item><title>时序数据库 InfluxDB（一）</title><link>https://rifewang.github.io/posts/influxdb/1/</link><pubDate>Fri, 25 Oct 2019 13:02:58 +0800</pubDate><guid>https://rifewang.github.io/posts/influxdb/1/</guid><description>数据库种类有很多，比如传统的关系型数据库 RDBMS（ 如 MySQL ），NoSQL 数据库（ 如 MongoDB ），Key-Value 类型（ 如 redis ），Wide column 类型（ 如 HBase ）等等等等，当然还有本系列文章将会介绍的时序数据库 TSDB（ 如 InfluxDB ）。
时序数据库 TSDB Link to heading 不同的数据库针对的应用场景有不同的偏重。TSDB（ time series database ）时序数据库是专门以时间维度进行设计和优化的。 TSDB 通常具有以下的特点：
时间是不可或缺的绝对主角（就像 MySQL 中的主键一样），数据按照时间顺序组织管理 高并发高吞吐量的数据写入 数据的更新很少发生 过期的数据可以批量删除 InfluxDB 就是一款非常优秀的时序数据库，高居 DB-Engines TSDB rank 榜首。
InfluxDB 分为免费的社区开源版本，以及需要收费的闭源商业版本，目前只有商业版本支持集群。
InfluxDB 的底层数据结构从 LSM 树到 B+ 树折腾了一通，最后自创了一个 TSM 树（ Time-Structured Merge Tree ），这也是它性能高且资源占用少的重要原因。
InfluxDB 由 go 语言编写而成，没有额外的依赖，它的查询语言 InfluxQL 与 SQL 极其相似，使用特别简单。
InfluxDB 基本概念 Link to heading InfluxDB 有以下几个核心概念： 1、database ： 数据库。</description></item></channel></rss>